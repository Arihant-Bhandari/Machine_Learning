{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "PbF6vplH1fa9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"housing_price_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "NxOCK1wuOmUW",
        "outputId": "f04b1718-ee00-4e2d-d69c-df9c971a8c58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SquareFeet</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2126</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1969</td>\n",
              "      <td>215355.283618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2459</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1980</td>\n",
              "      <td>195014.221626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1860</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1970</td>\n",
              "      <td>306891.012076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2294</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Urban</td>\n",
              "      <td>1996</td>\n",
              "      <td>206786.787153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2130</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>2001</td>\n",
              "      <td>272436.239065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>1282</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1975</td>\n",
              "      <td>100080.865895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>2854</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1988</td>\n",
              "      <td>374507.656727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>2979</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1962</td>\n",
              "      <td>384110.555590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>2596</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1984</td>\n",
              "      <td>380512.685957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>1572</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Rural</td>\n",
              "      <td>2011</td>\n",
              "      <td>221618.583218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SquareFeet  Bedrooms  Bathrooms Neighborhood  YearBuilt          Price\n",
              "0            2126         4          1        Rural       1969  215355.283618\n",
              "1            2459         3          2        Rural       1980  195014.221626\n",
              "2            1860         2          1       Suburb       1970  306891.012076\n",
              "3            2294         2          1        Urban       1996  206786.787153\n",
              "4            2130         5          2       Suburb       2001  272436.239065\n",
              "...           ...       ...        ...          ...        ...            ...\n",
              "49995        1282         5          3        Rural       1975  100080.865895\n",
              "49996        2854         2          2       Suburb       1988  374507.656727\n",
              "49997        2979         5          3       Suburb       1962  384110.555590\n",
              "49998        2596         5          2        Rural       1984  380512.685957\n",
              "49999        1572         5          3        Rural       2011  221618.583218\n",
              "\n",
              "[50000 rows x 6 columns]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TeCRUpf-ki2",
        "outputId": "79caa21d-7028-4706-9854-23ebe74c8388"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 6)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "KeQXFhuC110P",
        "outputId": "ca364a7a-d742-4054-d2ff-1489bb0007c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SquareFeet</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2126</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1969</td>\n",
              "      <td>215355.283618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2459</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1980</td>\n",
              "      <td>195014.221626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1860</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1970</td>\n",
              "      <td>306891.012076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2294</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Urban</td>\n",
              "      <td>1996</td>\n",
              "      <td>206786.787153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2130</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>2001</td>\n",
              "      <td>272436.239065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SquareFeet  Bedrooms  Bathrooms Neighborhood  YearBuilt          Price\n",
              "0        2126         4          1        Rural       1969  215355.283618\n",
              "1        2459         3          2        Rural       1980  195014.221626\n",
              "2        1860         2          1       Suburb       1970  306891.012076\n",
              "3        2294         2          1        Urban       1996  206786.787153\n",
              "4        2130         5          2       Suburb       2001  272436.239065"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "hGmGfZVkm4rE",
        "outputId": "962be8b8-763e-4982-cf03-4796f9d936ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SquareFeet</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>1282</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1975</td>\n",
              "      <td>100080.865895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>2854</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1988</td>\n",
              "      <td>374507.656727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>2979</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Suburb</td>\n",
              "      <td>1962</td>\n",
              "      <td>384110.555590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>2596</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Rural</td>\n",
              "      <td>1984</td>\n",
              "      <td>380512.685957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>1572</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>Rural</td>\n",
              "      <td>2011</td>\n",
              "      <td>221618.583218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       SquareFeet  Bedrooms  Bathrooms Neighborhood  YearBuilt          Price\n",
              "49995        1282         5          3        Rural       1975  100080.865895\n",
              "49996        2854         2          2       Suburb       1988  374507.656727\n",
              "49997        2979         5          3       Suburb       1962  384110.555590\n",
              "49998        2596         5          2        Rural       1984  380512.685957\n",
              "49999        1572         5          3        Rural       2011  221618.583218"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "OnF6QU0a1-Vp",
        "outputId": "93a88490-9323-4f65-da5c-d071e0ba510c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SquareFeet</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2006.374680</td>\n",
              "      <td>3.498700</td>\n",
              "      <td>1.995420</td>\n",
              "      <td>1985.404420</td>\n",
              "      <td>224827.325151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>575.513241</td>\n",
              "      <td>1.116326</td>\n",
              "      <td>0.815851</td>\n",
              "      <td>20.719377</td>\n",
              "      <td>76141.842966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>-36588.165397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1513.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1967.000000</td>\n",
              "      <td>169955.860225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2007.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1985.000000</td>\n",
              "      <td>225052.141166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2506.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2003.000000</td>\n",
              "      <td>279373.630052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2999.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2021.000000</td>\n",
              "      <td>492195.259972</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         SquareFeet      Bedrooms     Bathrooms     YearBuilt          Price\n",
              "count  50000.000000  50000.000000  50000.000000  50000.000000   50000.000000\n",
              "mean    2006.374680      3.498700      1.995420   1985.404420  224827.325151\n",
              "std      575.513241      1.116326      0.815851     20.719377   76141.842966\n",
              "min     1000.000000      2.000000      1.000000   1950.000000  -36588.165397\n",
              "25%     1513.000000      3.000000      1.000000   1967.000000  169955.860225\n",
              "50%     2007.000000      3.000000      2.000000   1985.000000  225052.141166\n",
              "75%     2506.000000      4.000000      3.000000   2003.000000  279373.630052\n",
              "max     2999.000000      5.000000      3.000000   2021.000000  492195.259972"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlR7G2Lvm4rF",
        "outputId": "c47b1600-8854-4a11-c227-9a72a2ae8a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   SquareFeet    50000 non-null  int64  \n",
            " 1   Bedrooms      50000 non-null  int64  \n",
            " 2   Bathrooms     50000 non-null  int64  \n",
            " 3   Neighborhood  50000 non-null  object \n",
            " 4   YearBuilt     50000 non-null  int64  \n",
            " 5   Price         50000 non-null  float64\n",
            "dtypes: float64(1), int64(4), object(1)\n",
            "memory usage: 2.3+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM9-8Z6Q2Afl",
        "outputId": "31166e0e-7933-4a68-fca4-f4ebbdd3976a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SquareFeet      0\n",
              "Bedrooms        0\n",
              "Bathrooms       0\n",
              "Neighborhood    0\n",
              "YearBuilt       0\n",
              "Price           0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Rural', 'Suburb', 'Urban'}"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(df['Neighborhood'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "booMa4XZm4rH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label = LabelEncoder()\n",
        "\n",
        "df['Neighborhood'] = label.fit_transform(df['Neighborhood'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0, 1, 2}"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(df['Neighborhood'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "1XCTTTPs2CVz",
        "outputId": "f4c7f243-655e-4618-ee5f-4fbfb31065f1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "P1EtDR5v2HBV"
      },
      "outputs": [],
      "source": [
        "x = df.drop(columns=['Price','YearBuilt'],axis=1)\n",
        "y = df['Price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-YbIJ9aYMztr",
        "outputId": "a222e7a7-15a9-493f-fc53-907525d0b0fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SquareFeet</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Neighborhood</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2126</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2459</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1860</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2294</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2130</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SquareFeet  Bedrooms  Bathrooms  Neighborhood\n",
              "0        2126         4          1             0\n",
              "1        2459         3          2             0\n",
              "2        1860         2          1             1\n",
              "3        2294         2          1             2\n",
              "4        2130         5          2             1"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "6Y86axSVm4rJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scale = MinMaxScaler()\n",
        "\n",
        "x = scale.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rcDXw7eM2zc",
        "outputId": "1a9ce3a1-b199-44ed-d4c9-3e89767f818d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Type after transfromation : <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(f'Data Type after transfromation : {type(x)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ar8yR8GbM5cl",
        "outputId": "2584f596-0b91-4c7c-b349-166bc12cf819"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.563282</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.729865</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.430215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.647324</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.565283</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1    2    3\n",
              "0  0.563282  0.666667  0.0  0.0\n",
              "1  0.729865  0.333333  0.5  0.0\n",
              "2  0.430215  0.000000  0.0  0.5\n",
              "3  0.647324  0.000000  0.0  1.0\n",
              "4  0.565283  1.000000  0.5  0.5"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = pd.DataFrame(x)\n",
        "\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWu7BQMSm4rJ"
      },
      "source": [
        "Splitting dataset into Training and Testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "AWE_8DGK2OGA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mj_dxy9NADv",
        "outputId": "0ad77cc4-2a2e-4fb5-edda-656d68c4bfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the data : (50000, 4)\n",
            "Size of the training data : (37500, 4)\n",
            "Size of the testing data : (12500, 4)\n"
          ]
        }
      ],
      "source": [
        "print(f'Size of the data : {x.shape}')\n",
        "print(f'Size of the training data : {x_train.shape}')\n",
        "print(f'Size of the testing data : {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "L9bhgg_32RC3"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "sabeTz6x2WKu",
        "outputId": "b838a67f-453c-461d-cbbe-fb879ae15f8f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "ibk-DaHm2YKX"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzeL90ta2b0O",
        "outputId": "10180fae-b08f-4540-92b5-7c005f3a02cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression RMSE: 49582.986048579565\n",
            "Linear Regression MSE: 2458472505.493636\n",
            "Linear Regression R-squared: 0.5737612335222099\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "print(\"Linear Regression RMSE:\", rmse)\n",
        "print(\"Linear Regression MSE:\", mse)\n",
        "print(\"Linear Regression R-squared:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpCVNa5R2pWo",
        "outputId": "77559335-0f92-40ec-b922-db295c5c5838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Price: 164836.87236394256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# features : SquareFeet, Bedrooms, Bathrooms, Neighborhood\n",
        "\n",
        "new_data = [[1400, 3, 3, 1]]\n",
        "predicted_price = model.predict(scale.transform(new_data))\n",
        "\n",
        "print(\"Predicted Price:\", predicted_price[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGB Regressor RMSE: 49931.81963960456\n",
            "XGB Regressor MSE: 2493186612.5219994\n",
            "XGB Regressor R-squared: 0.5677426597427253\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "xg_reg = XGBRegressor(objective ='reg:squarederror', \n",
        "                           colsample_bytree = 0.3, \n",
        "                           learning_rate = 0.1,\n",
        "                           max_depth = 5, \n",
        "                           alpha = 10, \n",
        "                           n_estimators = 100)\n",
        "\n",
        "xg_reg.fit(x_train, y_train)\n",
        "\n",
        "y_pred = xg_reg.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "print(\"XGB Regressor RMSE:\", rmse)\n",
        "print(\"XGB Regressor MSE:\", mse)\n",
        "print(\"XGB Regressor R-squared:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Price: 168166.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# features : SquareFeet, Bedrooms, Bathrooms, Neighborhood\n",
        "\n",
        "new_data = [[1400, 3, 3, 1]]\n",
        "predicted_price = xg_reg.predict(scale.transform(new_data))\n",
        "\n",
        "print(\"Predicted Price:\", predicted_price[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "mXMBiv5Nnw8O"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "r6I4AbRPnnH0"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGR4ltnl6_q1"
      },
      "source": [
        "Layer addition formula : model.add(type of layer(number of neurons, activation = __ , input_dim = __ )) wherein input_dim argument is for input layer only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwdXB5w5nk39",
        "outputId": "af8623b7-9580-4a31-88c6-bf5403551206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_33 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚           \u001b[38;5;34m160\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_34 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_35 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚         \u001b[38;5;34m8,320\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_36 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_37 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_38 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,961</span> (81.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,961\u001b[0m (81.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,961</span> (81.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,961\u001b[0m (81.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(Dense(32, activation='relu',input_dim = 4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1,activation = 'linear'))\n",
        "model.compile(loss = 'mean_squared_error',optimizer = 'nadam')\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNdjOoK_nvcz",
        "outputId": "48a7cad7-2c96-43c6-f920-26b46f5c589b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m283/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 45064040448.0000\n",
            "Epoch 1: val_loss improved from inf to 4383511552.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 41641779200.0000 - val_loss: 4383511552.0000\n",
            "Epoch 2/500\n",
            "\u001b[1m284/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 3881273856.0000\n",
            "Epoch 2: val_loss improved from 4383511552.00000 to 2575085312.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3790848256.0000 - val_loss: 2575085312.0000\n",
            "Epoch 3/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 2525082624.0000\n",
            "Epoch 3: val_loss improved from 2575085312.00000 to 2504336384.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2525064960.0000 - val_loss: 2504336384.0000\n",
            "Epoch 4/500\n",
            "\u001b[1m286/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 2468237056.0000\n",
            "Epoch 4: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2474607872.0000 - val_loss: 2505574656.0000\n",
            "Epoch 5/500\n",
            "\u001b[1m300/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 2529352960.0000\n",
            "Epoch 5: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527244544.0000 - val_loss: 2504686592.0000\n",
            "Epoch 6/500\n",
            "\u001b[1m326/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 2528667904.0000\n",
            "Epoch 6: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527789312.0000 - val_loss: 2506836224.0000\n",
            "Epoch 7/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 2448065024.0000\n",
            "Epoch 7: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2456060160.0000 - val_loss: 2508440576.0000\n",
            "Epoch 8/500\n",
            "\u001b[1m286/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2497857536.0000\n",
            "Epoch 8: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498098432.0000 - val_loss: 2526316800.0000\n",
            "Epoch 9/500\n",
            "\u001b[1m287/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 2475249664.0000\n",
            "Epoch 9: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480884736.0000 - val_loss: 2507472384.0000\n",
            "Epoch 10/500\n",
            "\u001b[1m287/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 2503636992.0000\n",
            "Epoch 10: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504834304.0000 - val_loss: 2513949440.0000\n",
            "Epoch 11/500\n",
            "\u001b[1m288/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 2509762048.0000\n",
            "Epoch 11: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510822656.0000 - val_loss: 2509569280.0000\n",
            "Epoch 12/500\n",
            "\u001b[1m301/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 2531287040.0000\n",
            "Epoch 12: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528984320.0000 - val_loss: 2509428224.0000\n",
            "Epoch 13/500\n",
            "\u001b[1m299/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 2492339456.0000\n",
            "Epoch 13: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493802240.0000 - val_loss: 2505045504.0000\n",
            "Epoch 14/500\n",
            "\u001b[1m298/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 2498116608.0000\n",
            "Epoch 14: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499782400.0000 - val_loss: 2512286976.0000\n",
            "Epoch 15/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 2509612032.0000\n",
            "Epoch 15: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509348608.0000 - val_loss: 2508275968.0000\n",
            "Epoch 16/500\n",
            "\u001b[1m283/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 2514574592.0000\n",
            "Epoch 16: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514235904.0000 - val_loss: 2520480512.0000\n",
            "Epoch 17/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 2497215744.0000\n",
            "Epoch 17: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498740992.0000 - val_loss: 2510600960.0000\n",
            "Epoch 18/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 2509808640.0000\n",
            "Epoch 18: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509807104.0000 - val_loss: 2505972224.0000\n",
            "Epoch 19/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 2472861952.0000\n",
            "Epoch 19: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2476229888.0000 - val_loss: 2504639744.0000\n",
            "Epoch 20/500\n",
            "\u001b[1m300/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 2498462976.0000\n",
            "Epoch 20: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500048896.0000 - val_loss: 2518268416.0000\n",
            "Epoch 21/500\n",
            "\u001b[1m299/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 2487157504.0000\n",
            "Epoch 21: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489027328.0000 - val_loss: 2517347584.0000\n",
            "Epoch 22/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 2472536064.0000\n",
            "Epoch 22: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2477748992.0000 - val_loss: 2524888320.0000\n",
            "Epoch 23/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 2508143872.0000\n",
            "Epoch 23: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508373504.0000 - val_loss: 2550451712.0000\n",
            "Epoch 24/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2479901952.0000\n",
            "Epoch 24: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2482406144.0000 - val_loss: 2515654144.0000\n",
            "Epoch 25/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2546582784.0000\n",
            "Epoch 25: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2543745024.0000 - val_loss: 2514580992.0000\n",
            "Epoch 26/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 2508940800.0000\n",
            "Epoch 26: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508959488.0000 - val_loss: 2514434816.0000\n",
            "Epoch 27/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 2514341888.0000\n",
            "Epoch 27: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513463040.0000 - val_loss: 2505180928.0000\n",
            "Epoch 28/500\n",
            "\u001b[1m298/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 2488753152.0000\n",
            "Epoch 28: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491586048.0000 - val_loss: 2504985088.0000\n",
            "Epoch 29/500\n",
            "\u001b[1m332/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 2510984192.0000\n",
            "Epoch 29: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510941184.0000 - val_loss: 2508421376.0000\n",
            "Epoch 30/500\n",
            "\u001b[1m327/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 2520069120.0000\n",
            "Epoch 30: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519826432.0000 - val_loss: 2504446208.0000\n",
            "Epoch 31/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 2511299840.0000\n",
            "Epoch 31: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510996480.0000 - val_loss: 2506301184.0000\n",
            "Epoch 32/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2515711488.0000\n",
            "Epoch 32: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515595008.0000 - val_loss: 2516416000.0000\n",
            "Epoch 33/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 2523914240.0000\n",
            "Epoch 33: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523744256.0000 - val_loss: 2511415296.0000\n",
            "Epoch 34/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 2514196480.0000\n",
            "Epoch 34: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513645568.0000 - val_loss: 2505891328.0000\n",
            "Epoch 35/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 2504930560.0000\n",
            "Epoch 35: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505181696.0000 - val_loss: 2512701696.0000\n",
            "Epoch 36/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 2531002112.0000\n",
            "Epoch 36: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527666176.0000 - val_loss: 2512447232.0000\n",
            "Epoch 37/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 2504549120.0000\n",
            "Epoch 37: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504894720.0000 - val_loss: 2506125824.0000\n",
            "Epoch 38/500\n",
            "\u001b[1m329/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 2493151232.0000\n",
            "Epoch 38: val_loss did not improve from 2504336384.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493714176.0000 - val_loss: 2511286528.0000\n",
            "Epoch 39/500\n",
            "\u001b[1m282/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 2487554048.0000\n",
            "Epoch 39: val_loss improved from 2504336384.00000 to 2503892224.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491170048.0000 - val_loss: 2503892224.0000\n",
            "Epoch 40/500\n",
            "\u001b[1m286/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 2517767424.0000\n",
            "Epoch 40: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515639552.0000 - val_loss: 2523498240.0000\n",
            "Epoch 41/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2507760640.0000\n",
            "Epoch 41: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508603392.0000 - val_loss: 2506187776.0000\n",
            "Epoch 42/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 2511450368.0000\n",
            "Epoch 42: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511464704.0000 - val_loss: 2510409216.0000\n",
            "Epoch 43/500\n",
            "\u001b[1m324/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 2502423808.0000\n",
            "Epoch 43: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502707968.0000 - val_loss: 2506976256.0000\n",
            "Epoch 44/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 2543152384.0000\n",
            "Epoch 44: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2538297600.0000 - val_loss: 2506498304.0000\n",
            "Epoch 45/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 2533508352.0000\n",
            "Epoch 45: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533084928.0000 - val_loss: 2535465472.0000\n",
            "Epoch 46/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 2520294656.0000\n",
            "Epoch 46: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519276288.0000 - val_loss: 2505172736.0000\n",
            "Epoch 47/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 2477485312.0000\n",
            "Epoch 47: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480751616.0000 - val_loss: 2517190144.0000\n",
            "Epoch 48/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 2522136832.0000\n",
            "Epoch 48: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520692992.0000 - val_loss: 2517174784.0000\n",
            "Epoch 49/500\n",
            "\u001b[1m299/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 2509416448.0000\n",
            "Epoch 49: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509838336.0000 - val_loss: 2516814080.0000\n",
            "Epoch 50/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 2503008512.0000\n",
            "Epoch 50: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503552000.0000 - val_loss: 2508346880.0000\n",
            "Epoch 51/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 2516955904.0000\n",
            "Epoch 51: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516119552.0000 - val_loss: 2505912832.0000\n",
            "Epoch 52/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 2507280384.0000\n",
            "Epoch 52: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507292672.0000 - val_loss: 2506137088.0000\n",
            "Epoch 53/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 2505719808.0000\n",
            "Epoch 53: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506222592.0000 - val_loss: 2510305792.0000\n",
            "Epoch 54/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 2490624256.0000\n",
            "Epoch 54: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492325376.0000 - val_loss: 2514148864.0000\n",
            "Epoch 55/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 2534684928.0000\n",
            "Epoch 55: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2532877568.0000 - val_loss: 2510774016.0000\n",
            "Epoch 56/500\n",
            "\u001b[1m301/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 2519159296.0000\n",
            "Epoch 56: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2518577664.0000 - val_loss: 2506723584.0000\n",
            "Epoch 57/500\n",
            "\u001b[1m301/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 2514603776.0000\n",
            "Epoch 57: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514256896.0000 - val_loss: 2509319936.0000\n",
            "Epoch 58/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 2522929152.0000\n",
            "Epoch 58: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521700352.0000 - val_loss: 2521107968.0000\n",
            "Epoch 59/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2516456192.0000\n",
            "Epoch 59: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516024064.0000 - val_loss: 2543533312.0000\n",
            "Epoch 60/500\n",
            "\u001b[1m281/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 2519840000.0000\n",
            "Epoch 60: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519001600.0000 - val_loss: 2507403776.0000\n",
            "Epoch 61/500\n",
            "\u001b[1m321/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 2536777216.0000\n",
            "Epoch 61: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535345152.0000 - val_loss: 2523591424.0000\n",
            "Epoch 62/500\n",
            "\u001b[1m281/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 2540770304.0000\n",
            "Epoch 62: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535924992.0000 - val_loss: 2509473024.0000\n",
            "Epoch 63/500\n",
            "\u001b[1m323/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 2481515520.0000\n",
            "Epoch 63: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2483007232.0000 - val_loss: 2515649280.0000\n",
            "Epoch 64/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 2506743552.0000\n",
            "Epoch 64: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506794240.0000 - val_loss: 2508531200.0000\n",
            "Epoch 65/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 2499471360.0000\n",
            "Epoch 65: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499715072.0000 - val_loss: 2507152128.0000\n",
            "Epoch 66/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 2511504128.0000\n",
            "Epoch 66: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511179008.0000 - val_loss: 2509535488.0000\n",
            "Epoch 67/500\n",
            "\u001b[1m325/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 2514705152.0000\n",
            "Epoch 67: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514562816.0000 - val_loss: 2512703232.0000\n",
            "Epoch 68/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536824320.0000\n",
            "Epoch 68: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2534667776.0000 - val_loss: 2506326272.0000\n",
            "Epoch 69/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2498241792.0000\n",
            "Epoch 69: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499009280.0000 - val_loss: 2508948736.0000\n",
            "Epoch 70/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 2524599040.0000\n",
            "Epoch 70: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524276736.0000 - val_loss: 2515214848.0000\n",
            "Epoch 71/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530387200.0000\n",
            "Epoch 71: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2530152960.0000 - val_loss: 2510650880.0000\n",
            "Epoch 72/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531249920.0000\n",
            "Epoch 72: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2529350656.0000 - val_loss: 2505436416.0000\n",
            "Epoch 73/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 2540610560.0000\n",
            "Epoch 73: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2537339392.0000 - val_loss: 2505930752.0000\n",
            "Epoch 74/500\n",
            "\u001b[1m296/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2565065472.0000\n",
            "Epoch 74: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2557923072.0000 - val_loss: 2509885952.0000\n",
            "Epoch 75/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2514786560.0000\n",
            "Epoch 75: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514188032.0000 - val_loss: 2510586368.0000\n",
            "Epoch 76/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2551238400.0000\n",
            "Epoch 76: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2547214848.0000 - val_loss: 2506951168.0000\n",
            "Epoch 77/500\n",
            "\u001b[1m321/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513693696.0000\n",
            "Epoch 77: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513429760.0000 - val_loss: 2531670016.0000\n",
            "Epoch 78/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529928960.0000\n",
            "Epoch 78: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528413184.0000 - val_loss: 2504899328.0000\n",
            "Epoch 79/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 2515410688.0000\n",
            "Epoch 79: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514772480.0000 - val_loss: 2504583168.0000\n",
            "Epoch 80/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2465652736.0000\n",
            "Epoch 80: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2469659648.0000 - val_loss: 2508697344.0000\n",
            "Epoch 81/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 2499814656.0000\n",
            "Epoch 81: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501554688.0000 - val_loss: 2513459456.0000\n",
            "Epoch 82/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2569260800.0000\n",
            "Epoch 82: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2564447488.0000 - val_loss: 2508427520.0000\n",
            "Epoch 83/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2550810368.0000\n",
            "Epoch 83: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2547413760.0000 - val_loss: 2506628352.0000\n",
            "Epoch 84/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2532622080.0000\n",
            "Epoch 84: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528682752.0000 - val_loss: 2505600768.0000\n",
            "Epoch 85/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 2517219584.0000\n",
            "Epoch 85: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516575232.0000 - val_loss: 2522257152.0000\n",
            "Epoch 86/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2500222976.0000\n",
            "Epoch 86: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501521408.0000 - val_loss: 2509282560.0000\n",
            "Epoch 87/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2514328832.0000\n",
            "Epoch 87: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513915904.0000 - val_loss: 2513088256.0000\n",
            "Epoch 88/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2540198144.0000\n",
            "Epoch 88: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2537441024.0000 - val_loss: 2505963520.0000\n",
            "Epoch 89/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2522010624.0000\n",
            "Epoch 89: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521115392.0000 - val_loss: 2507860736.0000\n",
            "Epoch 90/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2491209472.0000\n",
            "Epoch 90: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492838400.0000 - val_loss: 2509083136.0000\n",
            "Epoch 91/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524316416.0000\n",
            "Epoch 91: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524275712.0000 - val_loss: 2511393280.0000\n",
            "Epoch 92/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 2526214912.0000\n",
            "Epoch 92: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524918528.0000 - val_loss: 2504674304.0000\n",
            "Epoch 93/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 2535251712.0000\n",
            "Epoch 93: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533353216.0000 - val_loss: 2513202176.0000\n",
            "Epoch 94/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2544455936.0000\n",
            "Epoch 94: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2541115904.0000 - val_loss: 2506631424.0000\n",
            "Epoch 95/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 2454807552.0000\n",
            "Epoch 95: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2457916672.0000 - val_loss: 2506866176.0000\n",
            "Epoch 96/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2521010176.0000\n",
            "Epoch 96: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519571456.0000 - val_loss: 2543213568.0000\n",
            "Epoch 97/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522922496.0000\n",
            "Epoch 97: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521031168.0000 - val_loss: 2509222912.0000\n",
            "Epoch 98/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2502335488.0000\n",
            "Epoch 98: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502979328.0000 - val_loss: 2526019328.0000\n",
            "Epoch 99/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2499793408.0000\n",
            "Epoch 99: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500704768.0000 - val_loss: 2505759232.0000\n",
            "Epoch 100/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2530380032.0000\n",
            "Epoch 100: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528389632.0000 - val_loss: 2516156928.0000\n",
            "Epoch 101/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2515973120.0000\n",
            "Epoch 101: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515886080.0000 - val_loss: 2504965120.0000\n",
            "Epoch 102/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 2475137536.0000\n",
            "Epoch 102: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2477602048.0000 - val_loss: 2508160768.0000\n",
            "Epoch 103/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503648256.0000\n",
            "Epoch 103: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504860416.0000 - val_loss: 2543072512.0000\n",
            "Epoch 104/500\n",
            "\u001b[1m319/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2453332736.0000\n",
            "Epoch 104: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2456573440.0000 - val_loss: 2505743360.0000\n",
            "Epoch 105/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2516723200.0000\n",
            "Epoch 105: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516091648.0000 - val_loss: 2505377536.0000\n",
            "Epoch 106/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2519996928.0000\n",
            "Epoch 106: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519302656.0000 - val_loss: 2504279296.0000\n",
            "Epoch 107/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2522399232.0000\n",
            "Epoch 107: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521593088.0000 - val_loss: 2506123776.0000\n",
            "Epoch 108/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2516756224.0000\n",
            "Epoch 108: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516084992.0000 - val_loss: 2506637568.0000\n",
            "Epoch 109/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500407040.0000\n",
            "Epoch 109: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502919168.0000 - val_loss: 2505390080.0000\n",
            "Epoch 110/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2499043840.0000\n",
            "Epoch 110: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499923456.0000 - val_loss: 2505786624.0000\n",
            "Epoch 111/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2563885056.0000\n",
            "Epoch 111: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2559507712.0000 - val_loss: 2507252480.0000\n",
            "Epoch 112/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2537699584.0000\n",
            "Epoch 112: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535483904.0000 - val_loss: 2527677952.0000\n",
            "Epoch 113/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 2521252352.0000\n",
            "Epoch 113: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520479488.0000 - val_loss: 2505415680.0000\n",
            "Epoch 114/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 2495255040.0000\n",
            "Epoch 114: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496003584.0000 - val_loss: 2535934720.0000\n",
            "Epoch 115/500\n",
            "\u001b[1m332/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503909376.0000\n",
            "Epoch 115: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504035840.0000 - val_loss: 2509435904.0000\n",
            "Epoch 116/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2510329856.0000\n",
            "Epoch 116: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510481408.0000 - val_loss: 2504568832.0000\n",
            "Epoch 117/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2518082560.0000\n",
            "Epoch 117: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517264128.0000 - val_loss: 2525111808.0000\n",
            "Epoch 118/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 2495803136.0000\n",
            "Epoch 118: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496458496.0000 - val_loss: 2509026816.0000\n",
            "Epoch 119/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 2508687360.0000\n",
            "Epoch 119: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508887808.0000 - val_loss: 2515985920.0000\n",
            "Epoch 120/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508376832.0000\n",
            "Epoch 120: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508380672.0000 - val_loss: 2517729536.0000\n",
            "Epoch 121/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2543120384.0000\n",
            "Epoch 121: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2540526848.0000 - val_loss: 2510009344.0000\n",
            "Epoch 122/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 2478201856.0000\n",
            "Epoch 122: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481197056.0000 - val_loss: 2534975488.0000\n",
            "Epoch 123/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 2498511360.0000\n",
            "Epoch 123: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499674624.0000 - val_loss: 2504852992.0000\n",
            "Epoch 124/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2505843712.0000\n",
            "Epoch 124: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506291200.0000 - val_loss: 2523018240.0000\n",
            "Epoch 125/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491753216.0000\n",
            "Epoch 125: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491802112.0000 - val_loss: 2508069120.0000\n",
            "Epoch 126/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2512003072.0000\n",
            "Epoch 126: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511955200.0000 - val_loss: 2507903744.0000\n",
            "Epoch 127/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2530008320.0000\n",
            "Epoch 127: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528281088.0000 - val_loss: 2522662400.0000\n",
            "Epoch 128/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2533544192.0000\n",
            "Epoch 128: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531515392.0000 - val_loss: 2508125440.0000\n",
            "Epoch 129/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2507685888.0000\n",
            "Epoch 129: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507383808.0000 - val_loss: 2513741568.0000\n",
            "Epoch 130/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542964992.0000\n",
            "Epoch 130: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542592768.0000 - val_loss: 2507612928.0000\n",
            "Epoch 131/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2518298112.0000\n",
            "Epoch 131: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517389824.0000 - val_loss: 2505912064.0000\n",
            "Epoch 132/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2508113664.0000\n",
            "Epoch 132: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508110848.0000 - val_loss: 2512187648.0000\n",
            "Epoch 133/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 2479391232.0000\n",
            "Epoch 133: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481403136.0000 - val_loss: 2541509888.0000\n",
            "Epoch 134/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2500407808.0000\n",
            "Epoch 134: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500923136.0000 - val_loss: 2506137856.0000\n",
            "Epoch 135/500\n",
            "\u001b[1m293/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492877568.0000\n",
            "Epoch 135: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496658688.0000 - val_loss: 2505000192.0000\n",
            "Epoch 136/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2490085376.0000\n",
            "Epoch 136: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492426752.0000 - val_loss: 2505349120.0000\n",
            "Epoch 137/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 2524913408.0000\n",
            "Epoch 137: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524008960.0000 - val_loss: 2506059520.0000\n",
            "Epoch 138/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2490468352.0000\n",
            "Epoch 138: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492552704.0000 - val_loss: 2507803136.0000\n",
            "Epoch 139/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 2487064576.0000\n",
            "Epoch 139: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489103872.0000 - val_loss: 2512946688.0000\n",
            "Epoch 140/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526171392.0000\n",
            "Epoch 140: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2525409280.0000 - val_loss: 2507813632.0000\n",
            "Epoch 141/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2488162304.0000\n",
            "Epoch 141: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489864704.0000 - val_loss: 2507563264.0000\n",
            "Epoch 142/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2484533504.0000\n",
            "Epoch 142: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2486942464.0000 - val_loss: 2527286784.0000\n",
            "Epoch 143/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2491753728.0000\n",
            "Epoch 143: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492756736.0000 - val_loss: 2508273920.0000\n",
            "Epoch 144/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498855680.0000\n",
            "Epoch 144: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500754944.0000 - val_loss: 2511260160.0000\n",
            "Epoch 145/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2491587840.0000\n",
            "Epoch 145: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493372416.0000 - val_loss: 2509574656.0000\n",
            "Epoch 146/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 2463684352.0000\n",
            "Epoch 146: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2467288064.0000 - val_loss: 2509050112.0000\n",
            "Epoch 147/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 2475680256.0000\n",
            "Epoch 147: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2477484800.0000 - val_loss: 2516646656.0000\n",
            "Epoch 148/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2536260096.0000\n",
            "Epoch 148: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535171328.0000 - val_loss: 2515360000.0000\n",
            "Epoch 149/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2556438784.0000\n",
            "Epoch 149: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2550290944.0000 - val_loss: 2507815168.0000\n",
            "Epoch 150/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2487746304.0000\n",
            "Epoch 150: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489691904.0000 - val_loss: 2511177728.0000\n",
            "Epoch 151/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2508023296.0000\n",
            "Epoch 151: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508854016.0000 - val_loss: 2519452416.0000\n",
            "Epoch 152/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2520757760.0000\n",
            "Epoch 152: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519703552.0000 - val_loss: 2508346368.0000\n",
            "Epoch 153/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2509035008.0000\n",
            "Epoch 153: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509339136.0000 - val_loss: 2505845248.0000\n",
            "Epoch 154/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509207552.0000\n",
            "Epoch 154: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509766400.0000 - val_loss: 2508084992.0000\n",
            "Epoch 155/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2532085248.0000\n",
            "Epoch 155: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530330624.0000 - val_loss: 2507004928.0000\n",
            "Epoch 156/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2520822016.0000\n",
            "Epoch 156: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519955456.0000 - val_loss: 2509666816.0000\n",
            "Epoch 157/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 2499427328.0000\n",
            "Epoch 157: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500394752.0000 - val_loss: 2504651008.0000\n",
            "Epoch 158/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502388992.0000\n",
            "Epoch 158: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502506752.0000 - val_loss: 2504787200.0000\n",
            "Epoch 159/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2513251840.0000\n",
            "Epoch 159: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512630784.0000 - val_loss: 2517419008.0000\n",
            "Epoch 160/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 2498712832.0000\n",
            "Epoch 160: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499518720.0000 - val_loss: 2507770880.0000\n",
            "Epoch 161/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2500162816.0000\n",
            "Epoch 161: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500627712.0000 - val_loss: 2514975488.0000\n",
            "Epoch 162/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2553312512.0000\n",
            "Epoch 162: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2549661952.0000 - val_loss: 2509764608.0000\n",
            "Epoch 163/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2525552640.0000\n",
            "Epoch 163: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524431616.0000 - val_loss: 2524897792.0000\n",
            "Epoch 164/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2497117440.0000\n",
            "Epoch 164: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498369024.0000 - val_loss: 2505756160.0000\n",
            "Epoch 165/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 2549759488.0000\n",
            "Epoch 165: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2546973184.0000 - val_loss: 2507601408.0000\n",
            "Epoch 166/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2499059456.0000\n",
            "Epoch 166: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499776000.0000 - val_loss: 2517492736.0000\n",
            "Epoch 167/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 2522272512.0000\n",
            "Epoch 167: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521535744.0000 - val_loss: 2511290112.0000\n",
            "Epoch 168/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2486046208.0000\n",
            "Epoch 168: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487950592.0000 - val_loss: 2505457920.0000\n",
            "Epoch 169/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 2519817216.0000\n",
            "Epoch 169: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2518937344.0000 - val_loss: 2509041920.0000\n",
            "Epoch 170/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2497210624.0000\n",
            "Epoch 170: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498483712.0000 - val_loss: 2507211776.0000\n",
            "Epoch 171/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493590528.0000\n",
            "Epoch 171: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496082432.0000 - val_loss: 2504455936.0000\n",
            "Epoch 172/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501316096.0000  \n",
            "Epoch 172: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502832640.0000 - val_loss: 2505729280.0000\n",
            "Epoch 173/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2505352192.0000\n",
            "Epoch 173: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506087168.0000 - val_loss: 2513321472.0000\n",
            "Epoch 174/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 2487227136.0000\n",
            "Epoch 174: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488620544.0000 - val_loss: 2507243008.0000\n",
            "Epoch 175/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2545180928.0000\n",
            "Epoch 175: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2540059904.0000 - val_loss: 2509314560.0000\n",
            "Epoch 176/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2477666560.0000\n",
            "Epoch 176: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480171008.0000 - val_loss: 2514947328.0000\n",
            "Epoch 177/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 2503427584.0000\n",
            "Epoch 177: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503574528.0000 - val_loss: 2508565760.0000\n",
            "Epoch 178/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2500194816.0000\n",
            "Epoch 178: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501088256.0000 - val_loss: 2505625344.0000\n",
            "Epoch 179/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511220992.0000\n",
            "Epoch 179: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510900480.0000 - val_loss: 2509800960.0000\n",
            "Epoch 180/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2504338944.0000\n",
            "Epoch 180: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504807168.0000 - val_loss: 2514403584.0000\n",
            "Epoch 181/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2525426432.0000\n",
            "Epoch 181: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524345344.0000 - val_loss: 2535143168.0000\n",
            "Epoch 182/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2489528576.0000\n",
            "Epoch 182: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491830272.0000 - val_loss: 2506734336.0000\n",
            "Epoch 183/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 2489779968.0000\n",
            "Epoch 183: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491311872.0000 - val_loss: 2518585856.0000\n",
            "Epoch 184/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2531583488.0000\n",
            "Epoch 184: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529253888.0000 - val_loss: 2512991488.0000\n",
            "Epoch 185/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2516853504.0000\n",
            "Epoch 185: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516424704.0000 - val_loss: 2506690048.0000\n",
            "Epoch 186/500\n",
            "\u001b[1m293/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519954688.0000\n",
            "Epoch 186: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517738496.0000 - val_loss: 2509345536.0000\n",
            "Epoch 187/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509085184.0000\n",
            "Epoch 187: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509084928.0000 - val_loss: 2505182976.0000\n",
            "Epoch 188/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2522659328.0000\n",
            "Epoch 188: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521540608.0000 - val_loss: 2505569280.0000\n",
            "Epoch 189/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2510432000.0000\n",
            "Epoch 189: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510607872.0000 - val_loss: 2506992128.0000\n",
            "Epoch 190/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509317376.0000\n",
            "Epoch 190: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509314816.0000 - val_loss: 2508474880.0000\n",
            "Epoch 191/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 2510066432.0000\n",
            "Epoch 191: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510225664.0000 - val_loss: 2521179136.0000\n",
            "Epoch 192/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2482342144.0000\n",
            "Epoch 192: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2484625408.0000 - val_loss: 2505084416.0000\n",
            "Epoch 193/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 2497157376.0000\n",
            "Epoch 193: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497864960.0000 - val_loss: 2534221824.0000\n",
            "Epoch 194/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480415744.0000\n",
            "Epoch 194: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2484606720.0000 - val_loss: 2504196608.0000\n",
            "Epoch 195/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2523382528.0000\n",
            "Epoch 195: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522304256.0000 - val_loss: 2504701952.0000\n",
            "Epoch 196/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2498542336.0000\n",
            "Epoch 196: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499448576.0000 - val_loss: 2524306432.0000\n",
            "Epoch 197/500\n",
            "\u001b[1m298/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507294208.0000  \n",
            "Epoch 197: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507423744.0000 - val_loss: 2509740544.0000\n",
            "Epoch 198/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2518208768.0000\n",
            "Epoch 198: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516875008.0000 - val_loss: 2504966656.0000\n",
            "Epoch 199/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2482561792.0000\n",
            "Epoch 199: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2484440064.0000 - val_loss: 2529034752.0000\n",
            "Epoch 200/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 2509984768.0000\n",
            "Epoch 200: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509774080.0000 - val_loss: 2504609536.0000\n",
            "Epoch 201/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2482307840.0000\n",
            "Epoch 201: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2482721024.0000 - val_loss: 2505565696.0000\n",
            "Epoch 202/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2559162624.0000\n",
            "Epoch 202: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2555640832.0000 - val_loss: 2508345088.0000\n",
            "Epoch 203/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2519037952.0000\n",
            "Epoch 203: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517761024.0000 - val_loss: 2504547584.0000\n",
            "Epoch 204/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2489477888.0000\n",
            "Epoch 204: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2490886656.0000 - val_loss: 2504982272.0000\n",
            "Epoch 205/500\n",
            "\u001b[1m332/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496136960.0000\n",
            "Epoch 205: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496504064.0000 - val_loss: 2507236096.0000\n",
            "Epoch 206/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2493415424.0000\n",
            "Epoch 206: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494483712.0000 - val_loss: 2510001920.0000\n",
            "Epoch 207/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 2501967360.0000\n",
            "Epoch 207: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502604544.0000 - val_loss: 2505087232.0000\n",
            "Epoch 208/500\n",
            "\u001b[1m288/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2538934016.0000\n",
            "Epoch 208: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533588736.0000 - val_loss: 2515665152.0000\n",
            "Epoch 209/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2500572928.0000\n",
            "Epoch 209: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500830208.0000 - val_loss: 2514088192.0000\n",
            "Epoch 210/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2485193984.0000\n",
            "Epoch 210: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487431680.0000 - val_loss: 2519370496.0000\n",
            "Epoch 211/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515739392.0000\n",
            "Epoch 211: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514695168.0000 - val_loss: 2525830656.0000\n",
            "Epoch 212/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2539125504.0000\n",
            "Epoch 212: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536119040.0000 - val_loss: 2508325888.0000\n",
            "Epoch 213/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2544924160.0000\n",
            "Epoch 213: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2541266176.0000 - val_loss: 2515208448.0000\n",
            "Epoch 214/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2523492864.0000\n",
            "Epoch 214: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522431232.0000 - val_loss: 2506771968.0000\n",
            "Epoch 215/500\n",
            "\u001b[1m295/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2525254656.0000\n",
            "Epoch 215: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524311040.0000 - val_loss: 2508455680.0000\n",
            "Epoch 216/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2462594048.0000\n",
            "Epoch 216: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2466875392.0000 - val_loss: 2505882368.0000\n",
            "Epoch 217/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2513591040.0000\n",
            "Epoch 217: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513470464.0000 - val_loss: 2506963200.0000\n",
            "Epoch 218/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516972800.0000\n",
            "Epoch 218: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516929792.0000 - val_loss: 2527639808.0000\n",
            "Epoch 219/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2536783616.0000\n",
            "Epoch 219: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2534726656.0000 - val_loss: 2505269760.0000\n",
            "Epoch 220/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2506876160.0000\n",
            "Epoch 220: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506767360.0000 - val_loss: 2511777536.0000\n",
            "Epoch 221/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2503711488.0000\n",
            "Epoch 221: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503641856.0000 - val_loss: 2512960512.0000\n",
            "Epoch 222/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2522461184.0000\n",
            "Epoch 222: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521414144.0000 - val_loss: 2520313088.0000\n",
            "Epoch 223/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2517235200.0000\n",
            "Epoch 223: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516742400.0000 - val_loss: 2505660928.0000\n",
            "Epoch 224/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 2505418752.0000\n",
            "Epoch 224: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506202368.0000 - val_loss: 2506066176.0000\n",
            "Epoch 225/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500020480.0000\n",
            "Epoch 225: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501107968.0000 - val_loss: 2504812032.0000\n",
            "Epoch 226/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 2502025728.0000\n",
            "Epoch 226: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502429440.0000 - val_loss: 2518070784.0000\n",
            "Epoch 227/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 2505295104.0000\n",
            "Epoch 227: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505881856.0000 - val_loss: 2565503488.0000\n",
            "Epoch 228/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2508095488.0000\n",
            "Epoch 228: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508304384.0000 - val_loss: 2509536512.0000\n",
            "Epoch 229/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502907392.0000\n",
            "Epoch 229: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504332288.0000 - val_loss: 2505005568.0000\n",
            "Epoch 230/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 2480933888.0000\n",
            "Epoch 230: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2482707456.0000 - val_loss: 2527549696.0000\n",
            "Epoch 231/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 2499764992.0000\n",
            "Epoch 231: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500733440.0000 - val_loss: 2504829440.0000\n",
            "Epoch 232/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2560963584.0000  \n",
            "Epoch 232: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2554848768.0000 - val_loss: 2504466432.0000\n",
            "Epoch 233/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2496228864.0000\n",
            "Epoch 233: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497525504.0000 - val_loss: 2504478720.0000\n",
            "Epoch 234/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2516242688.0000\n",
            "Epoch 234: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515564032.0000 - val_loss: 2510761216.0000\n",
            "Epoch 235/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2486867200.0000\n",
            "Epoch 235: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488194304.0000 - val_loss: 2512754688.0000\n",
            "Epoch 236/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494800384.0000\n",
            "Epoch 236: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496115968.0000 - val_loss: 2505550080.0000\n",
            "Epoch 237/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2519586048.0000\n",
            "Epoch 237: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519075072.0000 - val_loss: 2506778112.0000\n",
            "Epoch 238/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2475762176.0000\n",
            "Epoch 238: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2479011584.0000 - val_loss: 2505910272.0000\n",
            "Epoch 239/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522258944.0000\n",
            "Epoch 239: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520611840.0000 - val_loss: 2509099520.0000\n",
            "Epoch 240/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2498934528.0000\n",
            "Epoch 240: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2499371008.0000 - val_loss: 2506526720.0000\n",
            "Epoch 241/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2545666816.0000\n",
            "Epoch 241: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542977792.0000 - val_loss: 2510364416.0000\n",
            "Epoch 242/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501954304.0000\n",
            "Epoch 242: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502079232.0000 - val_loss: 2514612480.0000\n",
            "Epoch 243/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2496679680.0000\n",
            "Epoch 243: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497500416.0000 - val_loss: 2505010176.0000\n",
            "Epoch 244/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2485624576.0000\n",
            "Epoch 244: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487654144.0000 - val_loss: 2505544704.0000\n",
            "Epoch 245/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2550769152.0000\n",
            "Epoch 245: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2545726720.0000 - val_loss: 2514449408.0000\n",
            "Epoch 246/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497795072.0000\n",
            "Epoch 246: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498793472.0000 - val_loss: 2525236736.0000\n",
            "Epoch 247/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2514574080.0000\n",
            "Epoch 247: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514311168.0000 - val_loss: 2512260352.0000\n",
            "Epoch 248/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481422336.0000\n",
            "Epoch 248: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2484555264.0000 - val_loss: 2507212544.0000\n",
            "Epoch 249/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2505462784.0000\n",
            "Epoch 249: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505610496.0000 - val_loss: 2505286656.0000\n",
            "Epoch 250/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 2523459072.0000\n",
            "Epoch 250: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521211392.0000 - val_loss: 2505537536.0000\n",
            "Epoch 251/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2538968320.0000\n",
            "Epoch 251: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536444416.0000 - val_loss: 2511991040.0000\n",
            "Epoch 252/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2503246592.0000\n",
            "Epoch 252: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503471872.0000 - val_loss: 2508361984.0000\n",
            "Epoch 253/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2515681536.0000\n",
            "Epoch 253: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514716416.0000 - val_loss: 2509938688.0000\n",
            "Epoch 254/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503718912.0000\n",
            "Epoch 254: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503781120.0000 - val_loss: 2514023680.0000\n",
            "Epoch 255/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2477021696.0000\n",
            "Epoch 255: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480174336.0000 - val_loss: 2504751872.0000\n",
            "Epoch 256/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2529598208.0000\n",
            "Epoch 256: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527873280.0000 - val_loss: 2504608768.0000\n",
            "Epoch 257/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2543377408.0000\n",
            "Epoch 257: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2537449728.0000 - val_loss: 2513176320.0000\n",
            "Epoch 258/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2517177088.0000\n",
            "Epoch 258: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516392960.0000 - val_loss: 2509900288.0000\n",
            "Epoch 259/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2492650240.0000\n",
            "Epoch 259: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494323968.0000 - val_loss: 2504997376.0000\n",
            "Epoch 260/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504733696.0000\n",
            "Epoch 260: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505836288.0000 - val_loss: 2509944832.0000\n",
            "Epoch 261/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2487441408.0000\n",
            "Epoch 261: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489318912.0000 - val_loss: 2505422336.0000\n",
            "Epoch 262/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2504847616.0000\n",
            "Epoch 262: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505182464.0000 - val_loss: 2507959296.0000\n",
            "Epoch 263/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494479616.0000\n",
            "Epoch 263: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495248896.0000 - val_loss: 2508978176.0000\n",
            "Epoch 264/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2533701376.0000\n",
            "Epoch 264: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531368192.0000 - val_loss: 2505081600.0000\n",
            "Epoch 265/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2502440448.0000\n",
            "Epoch 265: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503277312.0000 - val_loss: 2507779072.0000\n",
            "Epoch 266/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491048448.0000\n",
            "Epoch 266: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493049856.0000 - val_loss: 2511742208.0000\n",
            "Epoch 267/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2527761664.0000\n",
            "Epoch 267: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2525134848.0000 - val_loss: 2507919104.0000\n",
            "Epoch 268/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2509521920.0000\n",
            "Epoch 268: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509547776.0000 - val_loss: 2512195072.0000\n",
            "Epoch 269/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495728128.0000\n",
            "Epoch 269: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498226176.0000 - val_loss: 2505378304.0000\n",
            "Epoch 270/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 2523516416.0000\n",
            "Epoch 270: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522553856.0000 - val_loss: 2505310976.0000\n",
            "Epoch 271/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2519344896.0000\n",
            "Epoch 271: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519111680.0000 - val_loss: 2508836864.0000\n",
            "Epoch 272/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492566528.0000\n",
            "Epoch 272: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492663552.0000 - val_loss: 2505242112.0000\n",
            "Epoch 273/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2533744896.0000\n",
            "Epoch 273: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531391488.0000 - val_loss: 2506843648.0000\n",
            "Epoch 274/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 2497260800.0000\n",
            "Epoch 274: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498104320.0000 - val_loss: 2505441536.0000\n",
            "Epoch 275/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522134784.0000\n",
            "Epoch 275: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521996800.0000 - val_loss: 2509637376.0000\n",
            "Epoch 276/500\n",
            "\u001b[1m318/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 2511411200.0000\n",
            "Epoch 276: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511238144.0000 - val_loss: 2511747840.0000\n",
            "Epoch 277/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2505418752.0000\n",
            "Epoch 277: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505861376.0000 - val_loss: 2505827072.0000\n",
            "Epoch 278/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516619520.0000\n",
            "Epoch 278: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516599296.0000 - val_loss: 2513427968.0000\n",
            "Epoch 279/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2501384448.0000\n",
            "Epoch 279: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501800704.0000 - val_loss: 2504461824.0000\n",
            "Epoch 280/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2468737536.0000\n",
            "Epoch 280: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2474731264.0000 - val_loss: 2504688128.0000\n",
            "Epoch 281/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2489864960.0000\n",
            "Epoch 281: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491470592.0000 - val_loss: 2506430208.0000\n",
            "Epoch 282/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2511557120.0000\n",
            "Epoch 282: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511056640.0000 - val_loss: 2515780608.0000\n",
            "Epoch 283/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506615552.0000\n",
            "Epoch 283: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506649856.0000 - val_loss: 2504732160.0000\n",
            "Epoch 284/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2537275392.0000\n",
            "Epoch 284: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2534804224.0000 - val_loss: 2507053824.0000\n",
            "Epoch 285/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 2512952576.0000\n",
            "Epoch 285: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512895232.0000 - val_loss: 2511475200.0000\n",
            "Epoch 286/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527099136.0000\n",
            "Epoch 286: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526846976.0000 - val_loss: 2514766336.0000\n",
            "Epoch 287/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2490955776.0000\n",
            "Epoch 287: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492801280.0000 - val_loss: 2506241280.0000\n",
            "Epoch 288/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535833088.0000\n",
            "Epoch 288: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533308160.0000 - val_loss: 2506865664.0000\n",
            "Epoch 289/500\n",
            "\u001b[1m301/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526312704.0000  \n",
            "Epoch 289: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524731904.0000 - val_loss: 2504205824.0000\n",
            "Epoch 290/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2520210432.0000\n",
            "Epoch 290: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2518468352.0000 - val_loss: 2528219648.0000\n",
            "Epoch 291/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500070656.0000  \n",
            "Epoch 291: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500854784.0000 - val_loss: 2508538112.0000\n",
            "Epoch 292/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2484357888.0000\n",
            "Epoch 292: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2485899520.0000 - val_loss: 2513391616.0000\n",
            "Epoch 293/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2492858880.0000\n",
            "Epoch 293: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494907136.0000 - val_loss: 2504671232.0000\n",
            "Epoch 294/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508197888.0000  \n",
            "Epoch 294: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506889216.0000 - val_loss: 2504384000.0000\n",
            "Epoch 295/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2529407744.0000\n",
            "Epoch 295: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527258112.0000 - val_loss: 2506308608.0000\n",
            "Epoch 296/500\n",
            "\u001b[1m316/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2512273152.0000\n",
            "Epoch 296: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511980288.0000 - val_loss: 2505588992.0000\n",
            "Epoch 297/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504022272.0000\n",
            "Epoch 297: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2504124160.0000 - val_loss: 2512129280.0000\n",
            "Epoch 298/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 2472013056.0000\n",
            "Epoch 298: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2475302656.0000 - val_loss: 2506612736.0000\n",
            "Epoch 299/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2485628928.0000\n",
            "Epoch 299: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487586304.0000 - val_loss: 2507318784.0000\n",
            "Epoch 300/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533799936.0000\n",
            "Epoch 300: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533584640.0000 - val_loss: 2507293696.0000\n",
            "Epoch 301/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2513695232.0000\n",
            "Epoch 301: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513650176.0000 - val_loss: 2511862528.0000\n",
            "Epoch 302/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2508396800.0000\n",
            "Epoch 302: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508870656.0000 - val_loss: 2505734400.0000\n",
            "Epoch 303/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524195072.0000  \n",
            "Epoch 303: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522268928.0000 - val_loss: 2507526912.0000\n",
            "Epoch 304/500\n",
            "\u001b[1m314/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2504918272.0000\n",
            "Epoch 304: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505228800.0000 - val_loss: 2507739136.0000\n",
            "Epoch 305/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481548288.0000\n",
            "Epoch 305: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481719808.0000 - val_loss: 2509285888.0000\n",
            "Epoch 306/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 2491200512.0000\n",
            "Epoch 306: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492445184.0000 - val_loss: 2505304832.0000\n",
            "Epoch 307/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2469739776.0000\n",
            "Epoch 307: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2469971968.0000 - val_loss: 2514627840.0000\n",
            "Epoch 308/500\n",
            "\u001b[1m321/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527894784.0000\n",
            "Epoch 308: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526881024.0000 - val_loss: 2517859328.0000\n",
            "Epoch 309/500\n",
            "\u001b[1m331/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488754688.0000\n",
            "Epoch 309: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489337344.0000 - val_loss: 2505844736.0000\n",
            "Epoch 310/500\n",
            "\u001b[1m299/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496481280.0000\n",
            "Epoch 310: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497954048.0000 - val_loss: 2519940352.0000\n",
            "Epoch 311/500\n",
            "\u001b[1m321/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508804352.0000\n",
            "Epoch 311: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508856576.0000 - val_loss: 2514249216.0000\n",
            "Epoch 312/500\n",
            "\u001b[1m300/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524158976.0000\n",
            "Epoch 312: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522916352.0000 - val_loss: 2504520448.0000\n",
            "Epoch 313/500\n",
            "\u001b[1m319/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514729472.0000\n",
            "Epoch 313: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514432768.0000 - val_loss: 2524405504.0000\n",
            "Epoch 314/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544271872.0000\n",
            "Epoch 314: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2544173056.0000 - val_loss: 2505270016.0000\n",
            "Epoch 315/500\n",
            "\u001b[1m324/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496793088.0000\n",
            "Epoch 315: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497509888.0000 - val_loss: 2507299072.0000\n",
            "Epoch 316/500\n",
            "\u001b[1m321/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2490723328.0000\n",
            "Epoch 316: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492138240.0000 - val_loss: 2508233216.0000\n",
            "Epoch 317/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2470361600.0000\n",
            "Epoch 317: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2471091456.0000 - val_loss: 2536933888.0000\n",
            "Epoch 318/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529830144.0000\n",
            "Epoch 318: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526094336.0000 - val_loss: 2511580416.0000\n",
            "Epoch 319/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544380928.0000\n",
            "Epoch 319: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2541458432.0000 - val_loss: 2525295872.0000\n",
            "Epoch 320/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2495094784.0000\n",
            "Epoch 320: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496514304.0000 - val_loss: 2509233408.0000\n",
            "Epoch 321/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2528939264.0000\n",
            "Epoch 321: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526767360.0000 - val_loss: 2518898176.0000\n",
            "Epoch 322/500\n",
            "\u001b[1m288/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536897024.0000\n",
            "Epoch 322: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531990272.0000 - val_loss: 2509653760.0000\n",
            "Epoch 323/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2491747072.0000\n",
            "Epoch 323: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492972288.0000 - val_loss: 2508283648.0000\n",
            "Epoch 324/500\n",
            "\u001b[1m294/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480665600.0000\n",
            "Epoch 324: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2483738368.0000 - val_loss: 2505575168.0000\n",
            "Epoch 325/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2528861440.0000\n",
            "Epoch 325: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527048448.0000 - val_loss: 2550459392.0000\n",
            "Epoch 326/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2524236032.0000\n",
            "Epoch 326: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523230720.0000 - val_loss: 2507977472.0000\n",
            "Epoch 327/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533079808.0000\n",
            "Epoch 327: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530170368.0000 - val_loss: 2507039744.0000\n",
            "Epoch 328/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2517775104.0000\n",
            "Epoch 328: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516878336.0000 - val_loss: 2518456832.0000\n",
            "Epoch 329/500\n",
            "\u001b[1m296/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523353856.0000\n",
            "Epoch 329: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522574080.0000 - val_loss: 2511928576.0000\n",
            "Epoch 330/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 2548054784.0000\n",
            "Epoch 330: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544949248.0000 - val_loss: 2507082496.0000\n",
            "Epoch 331/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2567368448.0000\n",
            "Epoch 331: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2562761216.0000 - val_loss: 2510203904.0000\n",
            "Epoch 332/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2541509888.0000\n",
            "Epoch 332: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2538808832.0000 - val_loss: 2504234752.0000\n",
            "Epoch 333/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2483966976.0000\n",
            "Epoch 333: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2486011904.0000 - val_loss: 2505240832.0000\n",
            "Epoch 334/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2554351104.0000\n",
            "Epoch 334: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2553568512.0000 - val_loss: 2518583040.0000\n",
            "Epoch 335/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505507840.0000\n",
            "Epoch 335: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506740992.0000 - val_loss: 2506167040.0000\n",
            "Epoch 336/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544575232.0000\n",
            "Epoch 336: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544474368.0000 - val_loss: 2504933376.0000\n",
            "Epoch 337/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2537972992.0000\n",
            "Epoch 337: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535593216.0000 - val_loss: 2523325696.0000\n",
            "Epoch 338/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2524372224.0000\n",
            "Epoch 338: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523217408.0000 - val_loss: 2505132032.0000\n",
            "Epoch 339/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 2496603904.0000\n",
            "Epoch 339: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497494272.0000 - val_loss: 2504736256.0000\n",
            "Epoch 340/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2515238144.0000\n",
            "Epoch 340: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515031808.0000 - val_loss: 2515649792.0000\n",
            "Epoch 341/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2540018432.0000\n",
            "Epoch 341: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2539934976.0000 - val_loss: 2516918528.0000\n",
            "Epoch 342/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2484879360.0000\n",
            "Epoch 342: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487755776.0000 - val_loss: 2505349632.0000\n",
            "Epoch 343/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2547518208.0000\n",
            "Epoch 343: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2546942720.0000 - val_loss: 2505628928.0000\n",
            "Epoch 344/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2544146944.0000\n",
            "Epoch 344: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2540622592.0000 - val_loss: 2505804800.0000\n",
            "Epoch 345/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492062208.0000\n",
            "Epoch 345: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492226560.0000 - val_loss: 2505567488.0000\n",
            "Epoch 346/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2515075840.0000\n",
            "Epoch 346: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514391808.0000 - val_loss: 2517892096.0000\n",
            "Epoch 347/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514815744.0000\n",
            "Epoch 347: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514786048.0000 - val_loss: 2504679936.0000\n",
            "Epoch 348/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 2493138944.0000\n",
            "Epoch 348: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494254848.0000 - val_loss: 2508432896.0000\n",
            "Epoch 349/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 2514891776.0000\n",
            "Epoch 349: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514675456.0000 - val_loss: 2507318784.0000\n",
            "Epoch 350/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531031808.0000\n",
            "Epoch 350: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530903296.0000 - val_loss: 2504304384.0000\n",
            "Epoch 351/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2523551488.0000\n",
            "Epoch 351: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521871616.0000 - val_loss: 2517856768.0000\n",
            "Epoch 352/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515419392.0000\n",
            "Epoch 352: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515355648.0000 - val_loss: 2524927488.0000\n",
            "Epoch 353/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 2517029888.0000\n",
            "Epoch 353: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515911424.0000 - val_loss: 2526826240.0000\n",
            "Epoch 354/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515223296.0000\n",
            "Epoch 354: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515209728.0000 - val_loss: 2528111872.0000\n",
            "Epoch 355/500\n",
            "\u001b[1m317/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517821952.0000\n",
            "Epoch 355: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517171968.0000 - val_loss: 2507161088.0000\n",
            "Epoch 356/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488657664.0000\n",
            "Epoch 356: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488908800.0000 - val_loss: 2507652864.0000\n",
            "Epoch 357/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2494564608.0000\n",
            "Epoch 357: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496153600.0000 - val_loss: 2505693696.0000\n",
            "Epoch 358/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2500219904.0000\n",
            "Epoch 358: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501060608.0000 - val_loss: 2510852608.0000\n",
            "Epoch 359/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489182208.0000\n",
            "Epoch 359: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492000000.0000 - val_loss: 2525062144.0000\n",
            "Epoch 360/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 2506502400.0000\n",
            "Epoch 360: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506305792.0000 - val_loss: 2507604736.0000\n",
            "Epoch 361/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 2541246720.0000\n",
            "Epoch 361: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2539095808.0000 - val_loss: 2520046080.0000\n",
            "Epoch 362/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2505341184.0000\n",
            "Epoch 362: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505806848.0000 - val_loss: 2504909312.0000\n",
            "Epoch 363/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2500537088.0000\n",
            "Epoch 363: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501438720.0000 - val_loss: 2511762944.0000\n",
            "Epoch 364/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2504750336.0000\n",
            "Epoch 364: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505081600.0000 - val_loss: 2508714752.0000\n",
            "Epoch 365/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2514925824.0000\n",
            "Epoch 365: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514609408.0000 - val_loss: 2505259776.0000\n",
            "Epoch 366/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 2538759424.0000\n",
            "Epoch 366: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535826432.0000 - val_loss: 2577894400.0000\n",
            "Epoch 367/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2527956224.0000\n",
            "Epoch 367: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526582016.0000 - val_loss: 2509311232.0000\n",
            "Epoch 368/500\n",
            "\u001b[1m288/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519912960.0000\n",
            "Epoch 368: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517941504.0000 - val_loss: 2504182272.0000\n",
            "Epoch 369/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491091456.0000\n",
            "Epoch 369: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493075968.0000 - val_loss: 2527481088.0000\n",
            "Epoch 370/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523239424.0000\n",
            "Epoch 370: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2522955008.0000 - val_loss: 2517575680.0000\n",
            "Epoch 371/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2492956928.0000\n",
            "Epoch 371: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494048512.0000 - val_loss: 2507305728.0000\n",
            "Epoch 372/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509494016.0000\n",
            "Epoch 372: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509511168.0000 - val_loss: 2504471552.0000\n",
            "Epoch 373/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2485220864.0000\n",
            "Epoch 373: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487538688.0000 - val_loss: 2506319616.0000\n",
            "Epoch 374/500\n",
            "\u001b[1m292/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2557624832.0000\n",
            "Epoch 374: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2550993920.0000 - val_loss: 2515842560.0000\n",
            "Epoch 375/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2496655104.0000\n",
            "Epoch 375: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497790208.0000 - val_loss: 2504387840.0000\n",
            "Epoch 376/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527191552.0000\n",
            "Epoch 376: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524762624.0000 - val_loss: 2519159552.0000\n",
            "Epoch 377/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2493524736.0000\n",
            "Epoch 377: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494896896.0000 - val_loss: 2528891136.0000\n",
            "Epoch 378/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498051072.0000\n",
            "Epoch 378: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498149376.0000 - val_loss: 2538334720.0000\n",
            "Epoch 379/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511209216.0000  \n",
            "Epoch 379: val_loss did not improve from 2503892224.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510658048.0000 - val_loss: 2504597248.0000\n",
            "Epoch 380/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2531988224.0000\n",
            "Epoch 380: val_loss improved from 2503892224.00000 to 2503875328.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529898240.0000 - val_loss: 2503875328.0000\n",
            "Epoch 381/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2512791808.0000\n",
            "Epoch 381: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512343552.0000 - val_loss: 2508689408.0000\n",
            "Epoch 382/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502072064.0000\n",
            "Epoch 382: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502155264.0000 - val_loss: 2507609600.0000\n",
            "Epoch 383/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516498432.0000\n",
            "Epoch 383: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516335616.0000 - val_loss: 2505238528.0000\n",
            "Epoch 384/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2510442496.0000\n",
            "Epoch 384: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2510782976.0000 - val_loss: 2506551552.0000\n",
            "Epoch 385/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507457792.0000\n",
            "Epoch 385: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507533312.0000 - val_loss: 2510476544.0000\n",
            "Epoch 386/500\n",
            "\u001b[1m315/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2479190784.0000\n",
            "Epoch 386: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481378560.0000 - val_loss: 2538517504.0000\n",
            "Epoch 387/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2526797568.0000\n",
            "Epoch 387: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524880128.0000 - val_loss: 2504036096.0000\n",
            "Epoch 388/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 2489723648.0000\n",
            "Epoch 388: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491120896.0000 - val_loss: 2513523968.0000\n",
            "Epoch 389/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 2511944704.0000\n",
            "Epoch 389: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512203008.0000 - val_loss: 2537154048.0000\n",
            "Epoch 390/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2533898752.0000\n",
            "Epoch 390: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529983744.0000 - val_loss: 2505665024.0000\n",
            "Epoch 391/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2480970752.0000\n",
            "Epoch 391: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2483315968.0000 - val_loss: 2507023104.0000\n",
            "Epoch 392/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494548736.0000\n",
            "Epoch 392: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498475776.0000 - val_loss: 2504036096.0000\n",
            "Epoch 393/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 2497809408.0000\n",
            "Epoch 393: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498590208.0000 - val_loss: 2507500288.0000\n",
            "Epoch 394/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488808704.0000\n",
            "Epoch 394: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489112320.0000 - val_loss: 2506612480.0000\n",
            "Epoch 395/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 2498894336.0000 \n",
            "Epoch 395: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500417024.0000 - val_loss: 2504402176.0000\n",
            "Epoch 396/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528872448.0000\n",
            "Epoch 396: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527041280.0000 - val_loss: 2506003968.0000\n",
            "Epoch 397/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 2508931840.0000\n",
            "Epoch 397: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508299008.0000 - val_loss: 2508764416.0000\n",
            "Epoch 398/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496795136.0000\n",
            "Epoch 398: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497058048.0000 - val_loss: 2509628928.0000\n",
            "Epoch 399/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2483308544.0000\n",
            "Epoch 399: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2486186752.0000 - val_loss: 2508066816.0000\n",
            "Epoch 400/500\n",
            "\u001b[1m293/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502565376.0000\n",
            "Epoch 400: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503190272.0000 - val_loss: 2508932096.0000\n",
            "Epoch 401/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2500048384.0000\n",
            "Epoch 401: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500680192.0000 - val_loss: 2508034048.0000\n",
            "Epoch 402/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2485596672.0000\n",
            "Epoch 402: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2485796608.0000 - val_loss: 2505740032.0000\n",
            "Epoch 403/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2512237568.0000\n",
            "Epoch 403: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511427072.0000 - val_loss: 2507521792.0000\n",
            "Epoch 404/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488488192.0000\n",
            "Epoch 404: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488807680.0000 - val_loss: 2504352000.0000\n",
            "Epoch 405/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 2528854016.0000\n",
            "Epoch 405: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527425280.0000 - val_loss: 2507316224.0000\n",
            "Epoch 406/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513243648.0000\n",
            "Epoch 406: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513231360.0000 - val_loss: 2507757824.0000\n",
            "Epoch 407/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508822016.0000\n",
            "Epoch 407: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508918016.0000 - val_loss: 2504894976.0000\n",
            "Epoch 408/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505693696.0000\n",
            "Epoch 408: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505782272.0000 - val_loss: 2514962688.0000\n",
            "Epoch 409/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523228416.0000\n",
            "Epoch 409: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523150336.0000 - val_loss: 2504418304.0000\n",
            "Epoch 410/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2499307776.0000\n",
            "Epoch 410: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500063744.0000 - val_loss: 2507687936.0000\n",
            "Epoch 411/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496072192.0000\n",
            "Epoch 411: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496180992.0000 - val_loss: 2506343168.0000\n",
            "Epoch 412/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 2496766720.0000\n",
            "Epoch 412: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497427456.0000 - val_loss: 2518004224.0000\n",
            "Epoch 413/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508829440.0000\n",
            "Epoch 413: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508831232.0000 - val_loss: 2504057088.0000\n",
            "Epoch 414/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 2500690432.0000\n",
            "Epoch 414: val_loss did not improve from 2503875328.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500699904.0000 - val_loss: 2504151552.0000\n",
            "Epoch 415/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516157184.0000\n",
            "Epoch 415: val_loss improved from 2503875328.00000 to 2503577088.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516095744.0000 - val_loss: 2503577088.0000\n",
            "Epoch 416/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2478291968.0000\n",
            "Epoch 416: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2480373760.0000 - val_loss: 2516120576.0000\n",
            "Epoch 417/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495528192.0000\n",
            "Epoch 417: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495610112.0000 - val_loss: 2507298048.0000\n",
            "Epoch 418/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506347776.0000  \n",
            "Epoch 418: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506480128.0000 - val_loss: 2512677632.0000\n",
            "Epoch 419/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498357504.0000\n",
            "Epoch 419: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498419712.0000 - val_loss: 2504656896.0000\n",
            "Epoch 420/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2506836992.0000\n",
            "Epoch 420: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507241216.0000 - val_loss: 2504372736.0000\n",
            "Epoch 421/500\n",
            "\u001b[1m333/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487511552.0000\n",
            "Epoch 421: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488039936.0000 - val_loss: 2542338048.0000\n",
            "Epoch 422/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506798080.0000\n",
            "Epoch 422: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507020032.0000 - val_loss: 2507316992.0000\n",
            "Epoch 423/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505086208.0000\n",
            "Epoch 423: val_loss did not improve from 2503577088.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505162496.0000 - val_loss: 2516616704.0000\n",
            "Epoch 424/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2515771392.0000\n",
            "Epoch 424: val_loss improved from 2503577088.00000 to 2503387904.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515413760.0000 - val_loss: 2503387904.0000\n",
            "Epoch 425/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2521329152.0000\n",
            "Epoch 425: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520037888.0000 - val_loss: 2541800960.0000\n",
            "Epoch 426/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2471596288.0000\n",
            "Epoch 426: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2472147456.0000 - val_loss: 2507627264.0000\n",
            "Epoch 427/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2545405440.0000\n",
            "Epoch 427: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542631168.0000 - val_loss: 2514258176.0000\n",
            "Epoch 428/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2471962880.0000\n",
            "Epoch 428: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2477702656.0000 - val_loss: 2507717632.0000\n",
            "Epoch 429/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 2497142784.0000\n",
            "Epoch 429: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498556160.0000 - val_loss: 2507415040.0000\n",
            "Epoch 430/500\n",
            "\u001b[1m333/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487792896.0000\n",
            "Epoch 430: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488266752.0000 - val_loss: 2506729984.0000\n",
            "Epoch 431/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2560915456.0000\n",
            "Epoch 431: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2555631872.0000 - val_loss: 2510532352.0000\n",
            "Epoch 432/500\n",
            "\u001b[1m329/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520522240.0000\n",
            "Epoch 432: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520131840.0000 - val_loss: 2512754688.0000\n",
            "Epoch 433/500\n",
            "\u001b[1m312/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2555746304.0000\n",
            "Epoch 433: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2551101696.0000 - val_loss: 2513696256.0000\n",
            "Epoch 434/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2513422336.0000\n",
            "Epoch 434: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512788480.0000 - val_loss: 2508331264.0000\n",
            "Epoch 435/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501328384.0000\n",
            "Epoch 435: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501462272.0000 - val_loss: 2505364480.0000\n",
            "Epoch 436/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 2509586944.0000\n",
            "Epoch 436: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2508725760.0000 - val_loss: 2573008896.0000\n",
            "Epoch 437/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527407616.0000\n",
            "Epoch 437: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527140864.0000 - val_loss: 2507063552.0000\n",
            "Epoch 438/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 2513803264.0000\n",
            "Epoch 438: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513231616.0000 - val_loss: 2504404480.0000\n",
            "Epoch 439/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2550196992.0000\n",
            "Epoch 439: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2544506112.0000 - val_loss: 2524043008.0000\n",
            "Epoch 440/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 2524290304.0000\n",
            "Epoch 440: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523006464.0000 - val_loss: 2514631424.0000\n",
            "Epoch 441/500\n",
            "\u001b[1m291/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2518578176.0000\n",
            "Epoch 441: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517334016.0000 - val_loss: 2516909824.0000\n",
            "Epoch 442/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2503161856.0000\n",
            "Epoch 442: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503563264.0000 - val_loss: 2504363776.0000\n",
            "Epoch 443/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542975232.0000\n",
            "Epoch 443: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2542453760.0000 - val_loss: 2525717760.0000\n",
            "Epoch 444/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497618944.0000\n",
            "Epoch 444: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497659136.0000 - val_loss: 2505584640.0000\n",
            "Epoch 445/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2549519872.0000\n",
            "Epoch 445: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2546198016.0000 - val_loss: 2522220800.0000\n",
            "Epoch 446/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481084672.0000\n",
            "Epoch 446: val_loss did not improve from 2503387904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481421056.0000 - val_loss: 2514738176.0000\n",
            "Epoch 447/500\n",
            "\u001b[1m303/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506195968.0000\n",
            "Epoch 447: val_loss improved from 2503387904.00000 to 2503213056.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506647040.0000 - val_loss: 2503213056.0000\n",
            "Epoch 448/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 2534061056.0000\n",
            "Epoch 448: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531723520.0000 - val_loss: 2534989824.0000\n",
            "Epoch 449/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 2473774848.0000\n",
            "Epoch 449: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2477148928.0000 - val_loss: 2514168064.0000\n",
            "Epoch 450/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530970112.0000\n",
            "Epoch 450: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2530635776.0000 - val_loss: 2511568384.0000\n",
            "Epoch 451/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2511996416.0000\n",
            "Epoch 451: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2511877376.0000 - val_loss: 2546297088.0000\n",
            "Epoch 452/500\n",
            "\u001b[1m333/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498152448.0000\n",
            "Epoch 452: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498359040.0000 - val_loss: 2503533824.0000\n",
            "Epoch 453/500\n",
            "\u001b[1m337/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500812544.0000\n",
            "Epoch 453: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2500880128.0000 - val_loss: 2532788736.0000\n",
            "Epoch 454/500\n",
            "\u001b[1m311/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2550344960.0000\n",
            "Epoch 454: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2546315776.0000 - val_loss: 2505402112.0000\n",
            "Epoch 455/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519836160.0000\n",
            "Epoch 455: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519630336.0000 - val_loss: 2503888384.0000\n",
            "Epoch 456/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 2488962560.0000\n",
            "Epoch 456: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2490365952.0000 - val_loss: 2508287744.0000\n",
            "Epoch 457/500\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493881856.0000\n",
            "Epoch 457: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493924096.0000 - val_loss: 2508664320.0000\n",
            "Epoch 458/500\n",
            "\u001b[1m297/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517313024.0000  \n",
            "Epoch 458: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2516187648.0000 - val_loss: 2519333632.0000\n",
            "Epoch 459/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495225344.0000\n",
            "Epoch 459: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496559104.0000 - val_loss: 2511611392.0000\n",
            "Epoch 460/500\n",
            "\u001b[1m301/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2467972608.0000\n",
            "Epoch 460: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2472692736.0000 - val_loss: 2514498816.0000\n",
            "Epoch 461/500\n",
            "\u001b[1m333/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509198592.0000\n",
            "Epoch 461: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2509158912.0000 - val_loss: 2503899136.0000\n",
            "Epoch 462/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2507066624.0000\n",
            "Epoch 462: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2506898944.0000 - val_loss: 2513915648.0000\n",
            "Epoch 463/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 2520178688.0000\n",
            "Epoch 463: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519417600.0000 - val_loss: 2504118528.0000\n",
            "Epoch 464/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2537183232.0000\n",
            "Epoch 464: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536719616.0000 - val_loss: 2504274432.0000\n",
            "Epoch 465/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 2535283200.0000\n",
            "Epoch 465: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2532155904.0000 - val_loss: 2503682048.0000\n",
            "Epoch 466/500\n",
            "\u001b[1m329/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2490220544.0000\n",
            "Epoch 466: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2490867200.0000 - val_loss: 2503277824.0000\n",
            "Epoch 467/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2539574784.0000\n",
            "Epoch 467: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536652800.0000 - val_loss: 2530582272.0000\n",
            "Epoch 468/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498749952.0000\n",
            "Epoch 468: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2498882560.0000 - val_loss: 2515080960.0000\n",
            "Epoch 469/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 2500773888.0000\n",
            "Epoch 469: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2502198784.0000 - val_loss: 2508468736.0000\n",
            "Epoch 470/500\n",
            "\u001b[1m313/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 2500342784.0000\n",
            "Epoch 470: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501359872.0000 - val_loss: 2504554240.0000\n",
            "Epoch 471/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496461056.0000\n",
            "Epoch 471: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2496559872.0000 - val_loss: 2505550848.0000\n",
            "Epoch 472/500\n",
            "\u001b[1m310/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2502822144.0000\n",
            "Epoch 472: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503300096.0000 - val_loss: 2511186432.0000\n",
            "Epoch 473/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2491340288.0000\n",
            "Epoch 473: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494386432.0000 - val_loss: 2516102912.0000\n",
            "Epoch 474/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2531108096.0000\n",
            "Epoch 474: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2529014784.0000 - val_loss: 2514156544.0000\n",
            "Epoch 475/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 2513574144.0000\n",
            "Epoch 475: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513459712.0000 - val_loss: 2505299456.0000\n",
            "Epoch 476/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2524029184.0000\n",
            "Epoch 476: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523939584.0000 - val_loss: 2504733440.0000\n",
            "Epoch 477/500\n",
            "\u001b[1m309/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2524687616.0000\n",
            "Epoch 477: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523391488.0000 - val_loss: 2504199680.0000\n",
            "Epoch 478/500\n",
            "\u001b[1m289/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515525632.0000\n",
            "Epoch 478: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2515068160.0000 - val_loss: 2506574848.0000\n",
            "Epoch 479/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 2520842240.0000\n",
            "Epoch 479: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519260160.0000 - val_loss: 2517432832.0000\n",
            "Epoch 480/500\n",
            "\u001b[1m334/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497591040.0000\n",
            "Epoch 480: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2497773312.0000 - val_loss: 2510833152.0000\n",
            "Epoch 481/500\n",
            "\u001b[1m304/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2521248768.0000  \n",
            "Epoch 481: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519243520.0000 - val_loss: 2505337344.0000\n",
            "Epoch 482/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503928832.0000\n",
            "Epoch 482: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2503958528.0000 - val_loss: 2515602432.0000\n",
            "Epoch 483/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2538397440.0000\n",
            "Epoch 483: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2535489280.0000 - val_loss: 2507467264.0000\n",
            "Epoch 484/500\n",
            "\u001b[1m339/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526457344.0000\n",
            "Epoch 484: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526346752.0000 - val_loss: 2503771136.0000\n",
            "Epoch 485/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 2500348416.0000\n",
            "Epoch 485: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2501058048.0000 - val_loss: 2514830592.0000\n",
            "Epoch 486/500\n",
            "\u001b[1m328/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494107648.0000\n",
            "Epoch 486: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2494648064.0000 - val_loss: 2505744896.0000\n",
            "Epoch 487/500\n",
            "\u001b[1m302/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528828416.0000\n",
            "Epoch 487: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2526385664.0000 - val_loss: 2503710208.0000\n",
            "Epoch 488/500\n",
            "\u001b[1m332/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2486660864.0000\n",
            "Epoch 488: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2487277056.0000 - val_loss: 2504111872.0000\n",
            "Epoch 489/500\n",
            "\u001b[1m336/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505104896.0000\n",
            "Epoch 489: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2505178880.0000 - val_loss: 2514073344.0000\n",
            "Epoch 490/500\n",
            "\u001b[1m307/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 2515031296.0000\n",
            "Epoch 490: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2514764032.0000 - val_loss: 2505481728.0000\n",
            "Epoch 491/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488014592.0000\n",
            "Epoch 491: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2488188672.0000 - val_loss: 2504738048.0000\n",
            "Epoch 492/500\n",
            "\u001b[1m338/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513231360.0000\n",
            "Epoch 492: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2513179136.0000 - val_loss: 2509820928.0000\n",
            "Epoch 493/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 2536699648.0000\n",
            "Epoch 493: val_loss did not improve from 2503213056.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2534278912.0000 - val_loss: 2527545856.0000\n",
            "Epoch 494/500\n",
            "\u001b[1m332/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517824512.0000\n",
            "Epoch 494: val_loss improved from 2503213056.00000 to 2503195904.00000, saving model to best_model.keras\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2517555968.0000 - val_loss: 2503195904.0000\n",
            "Epoch 495/500\n",
            "\u001b[1m308/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 2511930880.0000\n",
            "Epoch 495: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2512169984.0000 - val_loss: 2514663936.0000\n",
            "Epoch 496/500\n",
            "\u001b[1m290/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507356672.0000\n",
            "Epoch 496: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507656448.0000 - val_loss: 2503788032.0000\n",
            "Epoch 497/500\n",
            "\u001b[1m329/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2481419008.0000\n",
            "Epoch 497: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2482305792.0000 - val_loss: 2516231424.0000\n",
            "Epoch 498/500\n",
            "\u001b[1m306/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 2491789056.0000\n",
            "Epoch 498: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493257472.0000 - val_loss: 2505884672.0000\n",
            "Epoch 499/500\n",
            "\u001b[1m335/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2492923648.0000\n",
            "Epoch 499: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2493184000.0000 - val_loss: 2509749760.0000\n",
            "Epoch 500/500\n",
            "\u001b[1m305/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 2483111168.0000\n",
            "Epoch 500: val_loss did not improve from 2503195904.00000\n",
            "\u001b[1m340/340\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2485933824.0000 - val_loss: 2514267648.0000\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train,y_train,epochs=500,batch_size=100,validation_split=0.2,callbacks = [model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "JkyCv5n9n_d-",
        "outputId": "3fd17b1d-d24b-4925-ca61-0dc15eae1440"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss') # plotting training and validation loss\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9rS_LQ5oCNS",
        "outputId": "ee51aa89-8dca-4506-ca59-82265b3106fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(\"best_model.keras\")\n",
        "\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94jzlCAoGZ8",
        "outputId": "dd483d78-c0ef-4f91-aded-db9157232f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANN RMSE: 49327.73359631698\n",
            "ANN MSE: 2433225301.749219\n",
            "ANN R-squared: 0.5799847052660851\n"
          ]
        }
      ],
      "source": [
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "print(\"ANN RMSE:\", rmse)\n",
        "print(\"ANN MSE:\", mse)\n",
        "print(\"ANN R-squared:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QURozOidtCNZ"
      },
      "source": [
        "Price prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH_dOSBNoLv-",
        "outputId": "789baa23-4b50-4cf6-b5f4-0c015dd809e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Predicted Price: [165393.64]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# features : SquareFeet, Bedrooms, Bathrooms, Neighborhood\n",
        "\n",
        "new_data = [[1400, 3, 3, 1]]\n",
        "predicted_price = model.predict(scale.transform(new_data))\n",
        "\n",
        "print(\"Predicted Price:\", predicted_price[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
