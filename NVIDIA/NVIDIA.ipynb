{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"nvda_stock_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>59.687500</td>\n",
       "      <td>59.977501</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>59.977501</td>\n",
       "      <td>59.741245</td>\n",
       "      <td>23753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>58.775002</td>\n",
       "      <td>59.457500</td>\n",
       "      <td>58.525002</td>\n",
       "      <td>59.017502</td>\n",
       "      <td>58.785023</td>\n",
       "      <td>20538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>58.080002</td>\n",
       "      <td>59.317501</td>\n",
       "      <td>57.817501</td>\n",
       "      <td>59.264999</td>\n",
       "      <td>59.031548</td>\n",
       "      <td>26263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>59.549999</td>\n",
       "      <td>60.442501</td>\n",
       "      <td>59.097500</td>\n",
       "      <td>59.982498</td>\n",
       "      <td>59.746220</td>\n",
       "      <td>31485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>59.939999</td>\n",
       "      <td>60.509998</td>\n",
       "      <td>59.537498</td>\n",
       "      <td>60.095001</td>\n",
       "      <td>59.858280</td>\n",
       "      <td>27710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>875.950012</td>\n",
       "      <td>879.919983</td>\n",
       "      <td>852.659973</td>\n",
       "      <td>877.570007</td>\n",
       "      <td>877.570007</td>\n",
       "      <td>38897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>872.400024</td>\n",
       "      <td>888.190002</td>\n",
       "      <td>863.000000</td>\n",
       "      <td>864.020020</td>\n",
       "      <td>864.020020</td>\n",
       "      <td>36370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>850.770020</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>812.549988</td>\n",
       "      <td>830.409973</td>\n",
       "      <td>830.409973</td>\n",
       "      <td>55986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>844.489990</td>\n",
       "      <td>862.369995</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>858.169983</td>\n",
       "      <td>858.169983</td>\n",
       "      <td>37789800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>877.890015</td>\n",
       "      <td>892.809998</td>\n",
       "      <td>870.400085</td>\n",
       "      <td>887.830017</td>\n",
       "      <td>887.830017</td>\n",
       "      <td>39834072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2020-01-02   59.687500   59.977501   59.180000   59.977501   59.741245   \n",
       "1     2020-01-03   58.775002   59.457500   58.525002   59.017502   58.785023   \n",
       "2     2020-01-06   58.080002   59.317501   57.817501   59.264999   59.031548   \n",
       "3     2020-01-07   59.549999   60.442501   59.097500   59.982498   59.746220   \n",
       "4     2020-01-08   59.939999   60.509998   59.537498   60.095001   59.858280   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1087  2024-04-29  875.950012  879.919983  852.659973  877.570007  877.570007   \n",
       "1088  2024-04-30  872.400024  888.190002  863.000000  864.020020  864.020020   \n",
       "1089  2024-05-01  850.770020  860.000000  812.549988  830.409973  830.409973   \n",
       "1090  2024-05-02  844.489990  862.369995  832.000000  858.169983  858.169983   \n",
       "1091  2024-05-03  877.890015  892.809998  870.400085  887.830017  887.830017   \n",
       "\n",
       "        Volume  \n",
       "0     23753600  \n",
       "1     20538400  \n",
       "2     26263600  \n",
       "3     31485600  \n",
       "4     27710800  \n",
       "...        ...  \n",
       "1087  38897100  \n",
       "1088  36370900  \n",
       "1089  55986300  \n",
       "1090  37789800  \n",
       "1091  39834072  \n",
       "\n",
       "[1092 rows x 7 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>59.687500</td>\n",
       "      <td>59.977501</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>59.977501</td>\n",
       "      <td>59.741245</td>\n",
       "      <td>23753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>58.775002</td>\n",
       "      <td>59.457500</td>\n",
       "      <td>58.525002</td>\n",
       "      <td>59.017502</td>\n",
       "      <td>58.785023</td>\n",
       "      <td>20538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>58.080002</td>\n",
       "      <td>59.317501</td>\n",
       "      <td>57.817501</td>\n",
       "      <td>59.264999</td>\n",
       "      <td>59.031548</td>\n",
       "      <td>26263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>59.549999</td>\n",
       "      <td>60.442501</td>\n",
       "      <td>59.097500</td>\n",
       "      <td>59.982498</td>\n",
       "      <td>59.746220</td>\n",
       "      <td>31485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>59.939999</td>\n",
       "      <td>60.509998</td>\n",
       "      <td>59.537498</td>\n",
       "      <td>60.095001</td>\n",
       "      <td>59.858280</td>\n",
       "      <td>27710800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2020-01-02  59.687500  59.977501  59.180000  59.977501  59.741245  23753600\n",
       "1  2020-01-03  58.775002  59.457500  58.525002  59.017502  58.785023  20538400\n",
       "2  2020-01-06  58.080002  59.317501  57.817501  59.264999  59.031548  26263600\n",
       "3  2020-01-07  59.549999  60.442501  59.097500  59.982498  59.746220  31485600\n",
       "4  2020-01-08  59.939999  60.509998  59.537498  60.095001  59.858280  27710800"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>875.950012</td>\n",
       "      <td>879.919983</td>\n",
       "      <td>852.659973</td>\n",
       "      <td>877.570007</td>\n",
       "      <td>877.570007</td>\n",
       "      <td>38897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>872.400024</td>\n",
       "      <td>888.190002</td>\n",
       "      <td>863.000000</td>\n",
       "      <td>864.020020</td>\n",
       "      <td>864.020020</td>\n",
       "      <td>36370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>850.770020</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>812.549988</td>\n",
       "      <td>830.409973</td>\n",
       "      <td>830.409973</td>\n",
       "      <td>55986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>844.489990</td>\n",
       "      <td>862.369995</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>858.169983</td>\n",
       "      <td>858.169983</td>\n",
       "      <td>37789800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>877.890015</td>\n",
       "      <td>892.809998</td>\n",
       "      <td>870.400085</td>\n",
       "      <td>887.830017</td>\n",
       "      <td>887.830017</td>\n",
       "      <td>39834072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "1087  2024-04-29  875.950012  879.919983  852.659973  877.570007  877.570007   \n",
       "1088  2024-04-30  872.400024  888.190002  863.000000  864.020020  864.020020   \n",
       "1089  2024-05-01  850.770020  860.000000  812.549988  830.409973  830.409973   \n",
       "1090  2024-05-02  844.489990  862.369995  832.000000  858.169983  858.169983   \n",
       "1091  2024-05-03  877.890015  892.809998  870.400085  887.830017  887.830017   \n",
       "\n",
       "        Volume  \n",
       "1087  38897100  \n",
       "1088  36370900  \n",
       "1089  55986300  \n",
       "1090  37789800  \n",
       "1091  39834072  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1092 entries, 0 to 1091\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       1092 non-null   object \n",
      " 1   Open       1092 non-null   float64\n",
      " 2   High       1092 non-null   float64\n",
      " 3   Low        1092 non-null   float64\n",
      " 4   Close      1092 non-null   float64\n",
      " 5   Adj Close  1092 non-null   float64\n",
      " 6   Volume     1092 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 59.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1.092000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>254.334991</td>\n",
       "      <td>259.101096</td>\n",
       "      <td>249.356678</td>\n",
       "      <td>254.482093</td>\n",
       "      <td>254.270355</td>\n",
       "      <td>4.682122e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>190.789638</td>\n",
       "      <td>194.100961</td>\n",
       "      <td>186.902226</td>\n",
       "      <td>190.612159</td>\n",
       "      <td>190.685090</td>\n",
       "      <td>1.869374e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.025002</td>\n",
       "      <td>52.485001</td>\n",
       "      <td>45.169998</td>\n",
       "      <td>49.099998</td>\n",
       "      <td>48.935833</td>\n",
       "      <td>9.788400e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.953754</td>\n",
       "      <td>134.927505</td>\n",
       "      <td>130.639996</td>\n",
       "      <td>133.048748</td>\n",
       "      <td>132.740437</td>\n",
       "      <td>3.431215e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>187.599998</td>\n",
       "      <td>191.900002</td>\n",
       "      <td>183.885002</td>\n",
       "      <td>187.995003</td>\n",
       "      <td>187.759239</td>\n",
       "      <td>4.470295e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>297.559990</td>\n",
       "      <td>304.597504</td>\n",
       "      <td>288.080002</td>\n",
       "      <td>295.994987</td>\n",
       "      <td>295.573479</td>\n",
       "      <td>5.692440e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>958.510010</td>\n",
       "      <td>974.000000</td>\n",
       "      <td>935.099976</td>\n",
       "      <td>950.020020</td>\n",
       "      <td>950.020020</td>\n",
       "      <td>1.543911e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close    Adj Close  \\\n",
       "count  1092.000000  1092.000000  1092.000000  1092.000000  1092.000000   \n",
       "mean    254.334991   259.101096   249.356678   254.482093   254.270355   \n",
       "std     190.789638   194.100961   186.902226   190.612159   190.685090   \n",
       "min      50.025002    52.485001    45.169998    49.099998    48.935833   \n",
       "25%     132.953754   134.927505   130.639996   133.048748   132.740437   \n",
       "50%     187.599998   191.900002   183.885002   187.995003   187.759239   \n",
       "75%     297.559990   304.597504   288.080002   295.994987   295.573479   \n",
       "max     958.510010   974.000000   935.099976   950.020020   950.020020   \n",
       "\n",
       "             Volume  \n",
       "count  1.092000e+03  \n",
       "mean   4.682122e+07  \n",
       "std    1.869374e+07  \n",
       "min    9.788400e+06  \n",
       "25%    3.431215e+07  \n",
       "50%    4.470295e+07  \n",
       "75%    5.692440e+07  \n",
       "max    1.543911e+08  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('closing price of stock')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Closing price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['Date'])):\n",
    "    s = ''.join(df['Date'][i].split(\"-\"))\n",
    "    df['Date'][i] = int(s)\n",
    "\n",
    "df['Date'] = df['Date'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1092 entries, 0 to 1091\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       1092 non-null   int64  \n",
      " 1   Open       1092 non-null   float64\n",
      " 2   High       1092 non-null   float64\n",
      " 3   Low        1092 non-null   float64\n",
      " 4   Close      1092 non-null   float64\n",
      " 5   Adj Close  1092 non-null   float64\n",
      " 6   Volume     1092 non-null   int64  \n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 59.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy().drop(columns=['Open'],axis=1)\n",
    "y = df.copy()[['Open']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "x = scale.fit_transform(x,y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape before reshaping : (873, 6)\n",
      "y_train shape before reshaping : (873, 1)\n",
      "x_test shape before reshaping : (219, 6)\n",
      "y_test shape before reshaping : (219, 1)\n",
      "x_train shape after reshaping : (873, 6, 1)\n",
      "x_test shape after reshaping : (219, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape before reshaping :\", x_train.shape)\n",
    "print(\"y_train shape before reshaping :\", y_train.shape)\n",
    "print(\"x_test shape before reshaping :\", x_test.shape)\n",
    "print(\"y_test shape before reshaping :\", y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "print(\"x_train shape after reshaping :\", x_train.shape)\n",
    "print(\"x_test shape after reshaping :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_69 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          â”‚         \u001b[38;5;34m4,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_70 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚        \u001b[38;5;34m24,832\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_71 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚        \u001b[38;5;34m98,816\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_72 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚       \u001b[38;5;34m131,584\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_73 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚       \u001b[38;5;34m394,240\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_74 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚       \u001b[38;5;34m525,312\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_75 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)         â”‚     \u001b[38;5;34m1,574,912\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_76 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m2,099,200\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_63 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m131,328\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_64 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚        \u001b[38;5;34m65,792\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_65 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_66 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_67 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_68 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,093,633</span> (19.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,093,633\u001b[0m (19.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,093,633</span> (19.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,093,633\u001b[0m (19.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,ELU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,return_sequences=True, activation=ELU(), input_shape = (x_train.shape[1],1)))\n",
    "model.add(LSTM(64,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(128,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(128,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(256,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(256,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(512,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(512,return_sequences=False, activation=ELU()))\n",
    "model.add(Dense(256, activation=ELU()))\n",
    "model.add(Dense(256, activation=ELU()))\n",
    "model.add(Dense(128, activation=ELU()))\n",
    "model.add(Dense(64, activation=ELU()))\n",
    "model.add(Dense(32, activation=ELU()))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 89753.9844\n",
      "Epoch 1: val_loss improved from inf to 27480.73438, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 171ms/step - loss: 88963.0312 - val_loss: 27480.7344\n",
      "Epoch 2/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 28967.7500\n",
      "Epoch 2: val_loss improved from 27480.73438 to 2413.10498, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 28642.7969 - val_loss: 2413.1050\n",
      "Epoch 3/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2544.6167\n",
      "Epoch 3: val_loss improved from 2413.10498 to 2412.37964, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 2544.7004 - val_loss: 2412.3796\n",
      "Epoch 4/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2397.8054\n",
      "Epoch 4: val_loss improved from 2412.37964 to 2276.29321, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 2401.7639 - val_loss: 2276.2932\n",
      "Epoch 5/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2503.1968\n",
      "Epoch 5: val_loss improved from 2276.29321 to 2240.44067, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 2502.3879 - val_loss: 2240.4407\n",
      "Epoch 6/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 3129.0225\n",
      "Epoch 6: val_loss improved from 2240.44067 to 2153.55054, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 3119.3618 - val_loss: 2153.5505\n",
      "Epoch 7/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2775.4170\n",
      "Epoch 7: val_loss improved from 2153.55054 to 2098.31128, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2773.5581 - val_loss: 2098.3113\n",
      "Epoch 8/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2651.5486\n",
      "Epoch 8: val_loss improved from 2098.31128 to 2086.75171, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2655.6704 - val_loss: 2086.7517\n",
      "Epoch 9/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2441.4351\n",
      "Epoch 9: val_loss did not improve from 2086.75171\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 2450.1423 - val_loss: 3075.6272\n",
      "Epoch 10/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2410.3311\n",
      "Epoch 10: val_loss did not improve from 2086.75171\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 2407.4783 - val_loss: 2313.6003\n",
      "Epoch 11/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2135.4885\n",
      "Epoch 11: val_loss improved from 2086.75171 to 1997.70703, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2136.6677 - val_loss: 1997.7070\n",
      "Epoch 12/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2507.0334\n",
      "Epoch 12: val_loss improved from 1997.70703 to 1876.13733, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 2505.4607 - val_loss: 1876.1373\n",
      "Epoch 13/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 2088.6453\n",
      "Epoch 13: val_loss did not improve from 1876.13733\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 2090.5396 - val_loss: 2176.0771\n",
      "Epoch 14/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2492.7561\n",
      "Epoch 14: val_loss improved from 1876.13733 to 1786.48193, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2488.8958 - val_loss: 1786.4819\n",
      "Epoch 15/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2054.8567\n",
      "Epoch 15: val_loss did not improve from 1786.48193\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 2056.2002 - val_loss: 1949.5035\n",
      "Epoch 16/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2302.8794\n",
      "Epoch 16: val_loss did not improve from 1786.48193\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 2324.2998 - val_loss: 2083.9436\n",
      "Epoch 17/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2037.7262\n",
      "Epoch 17: val_loss improved from 1786.48193 to 1649.57825, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2043.3024 - val_loss: 1649.5782\n",
      "Epoch 18/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1724.6567\n",
      "Epoch 18: val_loss improved from 1649.57825 to 1567.79919, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 1726.0778 - val_loss: 1567.7992\n",
      "Epoch 19/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1804.4194\n",
      "Epoch 19: val_loss improved from 1567.79919 to 1437.82581, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 1807.6604 - val_loss: 1437.8258\n",
      "Epoch 20/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1606.5486\n",
      "Epoch 20: val_loss did not improve from 1437.82581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1606.6477 - val_loss: 1455.6135\n",
      "Epoch 21/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1501.1473\n",
      "Epoch 21: val_loss improved from 1437.82581 to 1359.83032, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 1501.2043 - val_loss: 1359.8303\n",
      "Epoch 22/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1448.5052\n",
      "Epoch 22: val_loss improved from 1359.83032 to 1251.04004, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 1450.3346 - val_loss: 1251.0400\n",
      "Epoch 23/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1372.8234\n",
      "Epoch 23: val_loss did not improve from 1251.04004\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 1374.1694 - val_loss: 1322.4800\n",
      "Epoch 24/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1249.5989\n",
      "Epoch 24: val_loss improved from 1251.04004 to 1123.89661, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 1251.6393 - val_loss: 1123.8966\n",
      "Epoch 25/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1561.7349\n",
      "Epoch 25: val_loss did not improve from 1123.89661\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 1553.9894 - val_loss: 1251.5924\n",
      "Epoch 26/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1458.8225\n",
      "Epoch 26: val_loss did not improve from 1123.89661\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 1458.2556 - val_loss: 3484.7922\n",
      "Epoch 27/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1493.1663\n",
      "Epoch 27: val_loss did not improve from 1123.89661\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1482.4954 - val_loss: 1802.9834\n",
      "Epoch 28/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1053.4050\n",
      "Epoch 28: val_loss did not improve from 1123.89661\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 139ms/step - loss: 1058.2570 - val_loss: 4070.7361\n",
      "Epoch 29/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1399.3215\n",
      "Epoch 29: val_loss improved from 1123.89661 to 664.36176, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 1387.2820 - val_loss: 664.3618\n",
      "Epoch 30/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 863.4880\n",
      "Epoch 30: val_loss improved from 664.36176 to 357.31558, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 863.4209 - val_loss: 357.3156\n",
      "Epoch 31/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 566.0918\n",
      "Epoch 31: val_loss did not improve from 357.31558\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 565.6276 - val_loss: 383.2981\n",
      "Epoch 32/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 924.1420\n",
      "Epoch 32: val_loss did not improve from 357.31558\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 916.9146 - val_loss: 1076.1322\n",
      "Epoch 33/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 423.1126\n",
      "Epoch 33: val_loss did not improve from 357.31558\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 420.0407 - val_loss: 1846.2460\n",
      "Epoch 34/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 694.7798\n",
      "Epoch 34: val_loss improved from 357.31558 to 231.31195, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 691.5848 - val_loss: 231.3120\n",
      "Epoch 35/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 219.8375\n",
      "Epoch 35: val_loss did not improve from 231.31195\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 220.0972 - val_loss: 698.5326\n",
      "Epoch 36/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 878.9425\n",
      "Epoch 36: val_loss did not improve from 231.31195\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 867.9199 - val_loss: 318.7664\n",
      "Epoch 37/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 307.4655\n",
      "Epoch 37: val_loss did not improve from 231.31195\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 309.1861 - val_loss: 332.1710\n",
      "Epoch 38/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 414.9980\n",
      "Epoch 38: val_loss did not improve from 231.31195\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 413.3346 - val_loss: 332.2925\n",
      "Epoch 39/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 298.5774\n",
      "Epoch 39: val_loss did not improve from 231.31195\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 297.9804 - val_loss: 1221.2405\n",
      "Epoch 40/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 479.5780\n",
      "Epoch 40: val_loss improved from 231.31195 to 190.32668, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 473.4040 - val_loss: 190.3267\n",
      "Epoch 41/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 202.1791\n",
      "Epoch 41: val_loss improved from 190.32668 to 158.77896, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 205.6462 - val_loss: 158.7790\n",
      "Epoch 42/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 181.3793\n",
      "Epoch 42: val_loss did not improve from 158.77896\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 181.1333 - val_loss: 287.7583\n",
      "Epoch 43/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 196.1988\n",
      "Epoch 43: val_loss did not improve from 158.77896\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 198.4199 - val_loss: 231.2403\n",
      "Epoch 44/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 583.1612\n",
      "Epoch 44: val_loss did not improve from 158.77896\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 577.1935 - val_loss: 216.9060\n",
      "Epoch 45/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 244.3837\n",
      "Epoch 45: val_loss did not improve from 158.77896\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 243.5357 - val_loss: 197.3371\n",
      "Epoch 46/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 129.0167\n",
      "Epoch 46: val_loss improved from 158.77896 to 120.94764, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 128.5107 - val_loss: 120.9476\n",
      "Epoch 47/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 332.3033\n",
      "Epoch 47: val_loss did not improve from 120.94764\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 333.8719 - val_loss: 150.9706\n",
      "Epoch 48/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 230.9774\n",
      "Epoch 48: val_loss did not improve from 120.94764\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 231.1195 - val_loss: 148.2785\n",
      "Epoch 49/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 679.3610\n",
      "Epoch 49: val_loss did not improve from 120.94764\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 678.1152 - val_loss: 184.4655\n",
      "Epoch 50/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 230.7758\n",
      "Epoch 50: val_loss did not improve from 120.94764\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 231.0369 - val_loss: 249.3518\n",
      "Epoch 51/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 352.2231\n",
      "Epoch 51: val_loss did not improve from 120.94764\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 351.9115 - val_loss: 136.9441\n",
      "Epoch 52/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 147.6012\n",
      "Epoch 52: val_loss improved from 120.94764 to 119.91457, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 147.2652 - val_loss: 119.9146\n",
      "Epoch 53/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 251.7196\n",
      "Epoch 53: val_loss did not improve from 119.91457\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 251.8607 - val_loss: 215.4853\n",
      "Epoch 54/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 128.8060\n",
      "Epoch 54: val_loss did not improve from 119.91457\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 130.1197 - val_loss: 312.4054\n",
      "Epoch 55/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 132.6516\n",
      "Epoch 55: val_loss did not improve from 119.91457\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 134.9774 - val_loss: 844.7936\n",
      "Epoch 56/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 386.1072\n",
      "Epoch 56: val_loss improved from 119.91457 to 114.94319, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 382.3150 - val_loss: 114.9432\n",
      "Epoch 57/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 124.0604\n",
      "Epoch 57: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 131.3696 - val_loss: 2094.9570\n",
      "Epoch 58/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 3290.0427\n",
      "Epoch 58: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 3244.7983 - val_loss: 405.9852\n",
      "Epoch 59/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1240.9283\n",
      "Epoch 59: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 1228.7421 - val_loss: 221.0420\n",
      "Epoch 60/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 293.1720\n",
      "Epoch 60: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 295.3294 - val_loss: 186.3654\n",
      "Epoch 61/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 151.6338\n",
      "Epoch 61: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 151.6980 - val_loss: 355.6187\n",
      "Epoch 62/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 256.4641\n",
      "Epoch 62: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 255.4534 - val_loss: 283.7898\n",
      "Epoch 63/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 136.8078\n",
      "Epoch 63: val_loss did not improve from 114.94319\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 137.0137 - val_loss: 133.7228\n",
      "Epoch 64/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 215.1253\n",
      "Epoch 64: val_loss improved from 114.94319 to 107.67910, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 213.7081 - val_loss: 107.6791\n",
      "Epoch 65/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 95.3242\n",
      "Epoch 65: val_loss did not improve from 107.67910\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 95.8369 - val_loss: 463.1824\n",
      "Epoch 66/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 184.8568\n",
      "Epoch 66: val_loss improved from 107.67910 to 105.47302, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 184.7605 - val_loss: 105.4730\n",
      "Epoch 67/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 509.9883\n",
      "Epoch 67: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 518.6683 - val_loss: 204.3439\n",
      "Epoch 68/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 242.5672\n",
      "Epoch 68: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 243.7954 - val_loss: 114.9036\n",
      "Epoch 69/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 86.5666\n",
      "Epoch 69: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 87.1427 - val_loss: 145.7261\n",
      "Epoch 70/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 112.7798\n",
      "Epoch 70: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 112.6656 - val_loss: 172.3691\n",
      "Epoch 71/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 101.6243\n",
      "Epoch 71: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 102.6792 - val_loss: 184.0123\n",
      "Epoch 72/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 114.9213\n",
      "Epoch 72: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 115.9087 - val_loss: 138.7224\n",
      "Epoch 73/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 98.1718\n",
      "Epoch 73: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 99.0908 - val_loss: 112.2464\n",
      "Epoch 74/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 153.4231\n",
      "Epoch 74: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 153.0686 - val_loss: 353.0591\n",
      "Epoch 75/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 372.1046\n",
      "Epoch 75: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 381.0291 - val_loss: 553.5477\n",
      "Epoch 76/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 226.4493\n",
      "Epoch 76: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 224.3197 - val_loss: 151.2277\n",
      "Epoch 77/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 91.8415\n",
      "Epoch 77: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 91.4812 - val_loss: 173.7329\n",
      "Epoch 78/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 100.5179\n",
      "Epoch 78: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 100.4795 - val_loss: 136.2308\n",
      "Epoch 79/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 299.5103\n",
      "Epoch 79: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 297.9438 - val_loss: 188.5566\n",
      "Epoch 80/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 167.4274\n",
      "Epoch 80: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 166.6055 - val_loss: 121.3718\n",
      "Epoch 81/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 109.7538\n",
      "Epoch 81: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 110.7402 - val_loss: 180.7915\n",
      "Epoch 82/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 103.5967\n",
      "Epoch 82: val_loss did not improve from 105.47302\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 103.2500 - val_loss: 134.2477\n",
      "Epoch 83/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 110.9465\n",
      "Epoch 83: val_loss improved from 105.47302 to 84.52741, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 111.0780 - val_loss: 84.5274\n",
      "Epoch 84/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 81.8807\n",
      "Epoch 84: val_loss did not improve from 84.52741\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 84.1820 - val_loss: 696.1423\n",
      "Epoch 85/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 934.5021\n",
      "Epoch 85: val_loss did not improve from 84.52741\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 919.9641 - val_loss: 89.7869\n",
      "Epoch 86/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 146.0103\n",
      "Epoch 86: val_loss did not improve from 84.52741\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 146.6767 - val_loss: 125.2960\n",
      "Epoch 87/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 190.6826\n",
      "Epoch 87: val_loss did not improve from 84.52741\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 190.2107 - val_loss: 90.5634\n",
      "Epoch 88/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 97.9166\n",
      "Epoch 88: val_loss improved from 84.52741 to 80.29242, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 98.6149 - val_loss: 80.2924\n",
      "Epoch 89/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 227.1782\n",
      "Epoch 89: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 229.6679 - val_loss: 282.4267\n",
      "Epoch 90/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 157.5559\n",
      "Epoch 90: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 156.3437 - val_loss: 88.4111\n",
      "Epoch 91/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 92.6319\n",
      "Epoch 91: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 93.1590 - val_loss: 118.2619\n",
      "Epoch 92/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 424.4304\n",
      "Epoch 92: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 425.1538 - val_loss: 117.9877\n",
      "Epoch 93/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 86.7799\n",
      "Epoch 93: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 87.3961 - val_loss: 84.6160\n",
      "Epoch 94/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 103.3193\n",
      "Epoch 94: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 104.7541 - val_loss: 94.4068\n",
      "Epoch 95/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 71.5956\n",
      "Epoch 95: val_loss did not improve from 80.29242\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 71.7133 - val_loss: 130.0431\n",
      "Epoch 96/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 94.2510\n",
      "Epoch 96: val_loss improved from 80.29242 to 70.85396, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 94.3475 - val_loss: 70.8540\n",
      "Epoch 97/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 111.5047\n",
      "Epoch 97: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 112.9416 - val_loss: 109.6384\n",
      "Epoch 98/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 87.2254\n",
      "Epoch 98: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - loss: 88.9537 - val_loss: 232.3897\n",
      "Epoch 99/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 177.5412\n",
      "Epoch 99: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 177.7557 - val_loss: 192.0474\n",
      "Epoch 100/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 147.1120\n",
      "Epoch 100: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 147.5211 - val_loss: 77.2785\n",
      "Epoch 101/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 187.6101\n",
      "Epoch 101: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 189.2837 - val_loss: 76.4957\n",
      "Epoch 102/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 123.0156\n",
      "Epoch 102: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 123.6411 - val_loss: 170.0463\n",
      "Epoch 103/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 78.2544\n",
      "Epoch 103: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 78.6005 - val_loss: 263.9335\n",
      "Epoch 104/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 370.5753\n",
      "Epoch 104: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - loss: 372.1731 - val_loss: 137.3423\n",
      "Epoch 105/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 180.4201\n",
      "Epoch 105: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 182.4700 - val_loss: 90.3160\n",
      "Epoch 106/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 101.2269\n",
      "Epoch 106: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 101.8923 - val_loss: 132.9325\n",
      "Epoch 107/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 121.1477\n",
      "Epoch 107: val_loss did not improve from 70.85396\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 120.7525 - val_loss: 83.9330\n",
      "Epoch 108/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 129.7524\n",
      "Epoch 108: val_loss improved from 70.85396 to 65.73789, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - loss: 129.2811 - val_loss: 65.7379\n",
      "Epoch 109/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 173.1076\n",
      "Epoch 109: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - loss: 171.7193 - val_loss: 171.4006\n",
      "Epoch 110/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 86.0669\n",
      "Epoch 110: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 87.5620 - val_loss: 84.5894\n",
      "Epoch 111/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 135.1228\n",
      "Epoch 111: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - loss: 137.5853 - val_loss: 99.7901\n",
      "Epoch 112/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 115.8171\n",
      "Epoch 112: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 116.2444 - val_loss: 105.9860\n",
      "Epoch 113/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 186.7022\n",
      "Epoch 113: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - loss: 185.5934 - val_loss: 89.1331\n",
      "Epoch 114/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 111.7517\n",
      "Epoch 114: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 111.1133 - val_loss: 128.5750\n",
      "Epoch 115/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 171.3843\n",
      "Epoch 115: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 171.8943 - val_loss: 74.4343\n",
      "Epoch 116/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 136.5065\n",
      "Epoch 116: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 139.4785 - val_loss: 154.2824\n",
      "Epoch 117/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 131.6301\n",
      "Epoch 117: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 132.2872 - val_loss: 74.8334\n",
      "Epoch 118/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 116.2308\n",
      "Epoch 118: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 125.0039 - val_loss: 1266.5070\n",
      "Epoch 119/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 412.9822\n",
      "Epoch 119: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 414.0837 - val_loss: 121.0832\n",
      "Epoch 120/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 151.3785\n",
      "Epoch 120: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 150.9974 - val_loss: 77.8536\n",
      "Epoch 121/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 86.7938\n",
      "Epoch 121: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - loss: 86.8500 - val_loss: 166.2018\n",
      "Epoch 122/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 78.8291\n",
      "Epoch 122: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 79.6364 - val_loss: 95.5121\n",
      "Epoch 123/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 116.6650\n",
      "Epoch 123: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 117.5980 - val_loss: 117.7322\n",
      "Epoch 124/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 106.7206\n",
      "Epoch 124: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 106.6892 - val_loss: 79.3523\n",
      "Epoch 125/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 68.8785\n",
      "Epoch 125: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 69.6545 - val_loss: 102.1954\n",
      "Epoch 126/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 96.6364\n",
      "Epoch 126: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 96.7489 - val_loss: 112.3404\n",
      "Epoch 127/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 94.2580\n",
      "Epoch 127: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 93.6457 - val_loss: 71.8127\n",
      "Epoch 128/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 68.9349\n",
      "Epoch 128: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 69.7212 - val_loss: 66.6107\n",
      "Epoch 129/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 317.7408\n",
      "Epoch 129: val_loss did not improve from 65.73789\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 315.8696 - val_loss: 69.3064\n",
      "Epoch 130/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 105.0543\n",
      "Epoch 130: val_loss improved from 65.73789 to 62.05337, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - loss: 105.0736 - val_loss: 62.0534\n",
      "Epoch 131/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 106.7152\n",
      "Epoch 131: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 111.4422 - val_loss: 1290.1648\n",
      "Epoch 132/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 331.8448\n",
      "Epoch 132: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 331.1770 - val_loss: 169.8743\n",
      "Epoch 133/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 119.2931\n",
      "Epoch 133: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 118.7174 - val_loss: 275.9525\n",
      "Epoch 134/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 197.3385\n",
      "Epoch 134: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 198.7215 - val_loss: 79.4371\n",
      "Epoch 135/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 127.0657\n",
      "Epoch 135: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 126.4596 - val_loss: 69.3436\n",
      "Epoch 136/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 90.6256\n",
      "Epoch 136: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 91.0745 - val_loss: 98.1977\n",
      "Epoch 137/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 57.4534\n",
      "Epoch 137: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 57.4985 - val_loss: 126.3816\n",
      "Epoch 138/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 121.7401\n",
      "Epoch 138: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 120.9323 - val_loss: 183.3817\n",
      "Epoch 139/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 185.1070\n",
      "Epoch 139: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 185.0630 - val_loss: 143.0486\n",
      "Epoch 140/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 166.2293\n",
      "Epoch 140: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 166.3775 - val_loss: 94.5832\n",
      "Epoch 141/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 124.2165\n",
      "Epoch 141: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 125.0691 - val_loss: 81.1213\n",
      "Epoch 142/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 67.0414\n",
      "Epoch 142: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 67.1546 - val_loss: 84.3127\n",
      "Epoch 143/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 305.3062\n",
      "Epoch 143: val_loss did not improve from 62.05337\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 305.0394 - val_loss: 66.7565\n",
      "Epoch 144/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 70.3499\n",
      "Epoch 144: val_loss improved from 62.05337 to 62.04581, saving model to best_model.keras\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - loss: 70.1333 - val_loss: 62.0458\n",
      "Epoch 145/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 87.4655\n",
      "Epoch 145: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 88.6069 - val_loss: 67.2532\n",
      "Epoch 146/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 228.5004\n",
      "Epoch 146: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 226.6823 - val_loss: 73.7103\n",
      "Epoch 147/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 59.5494\n",
      "Epoch 147: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 59.7223 - val_loss: 103.5097\n",
      "Epoch 148/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 175.0436\n",
      "Epoch 148: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 176.1745 - val_loss: 64.1341\n",
      "Epoch 149/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 80.9833\n",
      "Epoch 149: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 80.9766 - val_loss: 107.3426\n",
      "Epoch 150/150\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 164.9694\n",
      "Epoch 150: val_loss did not improve from 62.04581\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - loss: 164.2040 - val_loss: 79.6840\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,epochs=150, batch_size=25, verbose=1, validation_data=(x_test,y_test), callbacks= model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy().drop(columns=['Close'],axis=1)\n",
    "y = df.copy()[['Close']]\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "x = scale.fit_transform(x,y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape before reshaping : (819, 6)\n",
      "y_train shape before reshaping : (819, 1)\n",
      "x_test shape before reshaping : (273, 6)\n",
      "y_test shape before reshaping : (273, 1)\n",
      "x_train shape after reshaping : (819, 6, 1)\n",
      "x_test shape after reshaping : (273, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape before reshaping :\", x_train.shape)\n",
    "print(\"y_train shape before reshaping :\", y_train.shape)\n",
    "print(\"x_test shape before reshaping :\", x_test.shape)\n",
    "print(\"y_test shape before reshaping :\", y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "print(\"x_train shape after reshaping :\", x_train.shape)\n",
    "print(\"x_test shape after reshaping :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_77 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          â”‚         \u001b[38;5;34m4,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_78 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚        \u001b[38;5;34m24,832\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_79 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚        \u001b[38;5;34m98,816\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_80 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚       \u001b[38;5;34m131,584\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_81 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚       \u001b[38;5;34m394,240\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_82 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚       \u001b[38;5;34m525,312\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_83 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)         â”‚     \u001b[38;5;34m1,574,912\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_84 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m2,099,200\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_69 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m131,328\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_70 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚        \u001b[38;5;34m65,792\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_71 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_72 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_73 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_74 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,093,633</span> (19.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,093,633\u001b[0m (19.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,093,633</span> (19.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,093,633\u001b[0m (19.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,ELU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,return_sequences=True, activation=ELU(), input_shape = (x_train.shape[1],1)))\n",
    "model.add(LSTM(64,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(128,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(128,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(256,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(256,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(512,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(512,return_sequences=False, activation=ELU()))\n",
    "model.add(Dense(256, activation=ELU()))\n",
    "model.add(Dense(256, activation=ELU()))\n",
    "model.add(Dense(128, activation=ELU()))\n",
    "model.add(Dense(64, activation=ELU()))\n",
    "model.add(Dense(32, activation=ELU()))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 93422.6875\n",
      "Epoch 1: val_loss improved from inf to 28757.22070, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - loss: 92520.5938 - val_loss: 28757.2207\n",
      "Epoch 2/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 27974.8848\n",
      "Epoch 2: val_loss improved from 28757.22070 to 2895.72876, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 27562.1172 - val_loss: 2895.7288\n",
      "Epoch 3/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 3554.5039\n",
      "Epoch 3: val_loss improved from 2895.72876 to 2248.93530, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 3543.1912 - val_loss: 2248.9353\n",
      "Epoch 4/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2477.8616\n",
      "Epoch 4: val_loss did not improve from 2248.93530\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 2484.0471 - val_loss: 2462.3132\n",
      "Epoch 5/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2683.7183\n",
      "Epoch 5: val_loss did not improve from 2248.93530\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 2700.9275 - val_loss: 2577.2168\n",
      "Epoch 6/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2472.9272\n",
      "Epoch 6: val_loss did not improve from 2248.93530\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 2475.5154 - val_loss: 2548.6660\n",
      "Epoch 7/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 2595.1763\n",
      "Epoch 7: val_loss did not improve from 2248.93530\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 2598.3652 - val_loss: 2515.3452\n",
      "Epoch 8/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2945.4639\n",
      "Epoch 8: val_loss improved from 2248.93530 to 2136.34814, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2954.9475 - val_loss: 2136.3481\n",
      "Epoch 9/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2818.9329\n",
      "Epoch 9: val_loss improved from 2136.34814 to 2004.00732, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - loss: 2817.7300 - val_loss: 2004.0073\n",
      "Epoch 10/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 3010.5967\n",
      "Epoch 10: val_loss did not improve from 2004.00732\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 3002.4753 - val_loss: 3693.6970\n",
      "Epoch 11/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 3020.8240\n",
      "Epoch 11: val_loss did not improve from 2004.00732\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 3008.0632 - val_loss: 2265.9146\n",
      "Epoch 12/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2588.3433\n",
      "Epoch 12: val_loss improved from 2004.00732 to 1889.33911, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2592.3843 - val_loss: 1889.3391\n",
      "Epoch 13/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2498.4548\n",
      "Epoch 13: val_loss did not improve from 1889.33911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 2509.5557 - val_loss: 2204.1914\n",
      "Epoch 14/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 2099.1257\n",
      "Epoch 14: val_loss did not improve from 1889.33911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 2102.6504 - val_loss: 2048.8311\n",
      "Epoch 15/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 3145.4973\n",
      "Epoch 15: val_loss did not improve from 1889.33911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 3133.5347 - val_loss: 2464.4180\n",
      "Epoch 16/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 2273.4133\n",
      "Epoch 16: val_loss did not improve from 1889.33911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 2276.8870 - val_loss: 2318.4409\n",
      "Epoch 17/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 2148.6216\n",
      "Epoch 17: val_loss improved from 1889.33911 to 1701.76953, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 2149.6128 - val_loss: 1701.7695\n",
      "Epoch 18/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 2039.4821\n",
      "Epoch 18: val_loss improved from 1701.76953 to 1582.42542, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 2044.1448 - val_loss: 1582.4254\n",
      "Epoch 19/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1819.7000\n",
      "Epoch 19: val_loss did not improve from 1582.42542\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 1826.6659 - val_loss: 1891.5647\n",
      "Epoch 20/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1943.3503\n",
      "Epoch 20: val_loss improved from 1582.42542 to 1481.82300, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 1938.6234 - val_loss: 1481.8230\n",
      "Epoch 21/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1614.4771\n",
      "Epoch 21: val_loss improved from 1481.82300 to 1391.55859, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 1615.5077 - val_loss: 1391.5586\n",
      "Epoch 22/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1775.3315\n",
      "Epoch 22: val_loss improved from 1391.55859 to 1268.88550, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 1776.6398 - val_loss: 1268.8855\n",
      "Epoch 23/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1419.6732\n",
      "Epoch 23: val_loss improved from 1268.88550 to 1217.42896, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 1423.2454 - val_loss: 1217.4290\n",
      "Epoch 24/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1349.8528\n",
      "Epoch 24: val_loss did not improve from 1217.42896\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 1349.9764 - val_loss: 1308.2466\n",
      "Epoch 25/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1413.9196\n",
      "Epoch 25: val_loss did not improve from 1217.42896\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 1416.6255 - val_loss: 1554.1071\n",
      "Epoch 26/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1662.3346\n",
      "Epoch 26: val_loss did not improve from 1217.42896\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 1658.9412 - val_loss: 5377.9473\n",
      "Epoch 27/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 3222.8569\n",
      "Epoch 27: val_loss did not improve from 1217.42896\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 3188.7905 - val_loss: 1724.1038\n",
      "Epoch 28/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1554.9487\n",
      "Epoch 28: val_loss improved from 1217.42896 to 887.02368, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 1547.5491 - val_loss: 887.0237\n",
      "Epoch 29/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 912.5621\n",
      "Epoch 29: val_loss improved from 887.02368 to 684.39850, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 915.3715 - val_loss: 684.3985\n",
      "Epoch 30/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 588.6614\n",
      "Epoch 30: val_loss improved from 684.39850 to 563.30054, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 589.8323 - val_loss: 563.3005\n",
      "Epoch 31/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1236.2844\n",
      "Epoch 31: val_loss did not improve from 563.30054\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 1262.0637 - val_loss: 1129.3505\n",
      "Epoch 32/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1166.6913\n",
      "Epoch 32: val_loss did not improve from 563.30054\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1161.1947 - val_loss: 1577.6864\n",
      "Epoch 33/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 2861.2090\n",
      "Epoch 33: val_loss improved from 563.30054 to 355.84012, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 2824.5334 - val_loss: 355.8401\n",
      "Epoch 34/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 429.2780\n",
      "Epoch 34: val_loss improved from 355.84012 to 234.35271, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 429.3568 - val_loss: 234.3527\n",
      "Epoch 35/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 743.7507\n",
      "Epoch 35: val_loss did not improve from 234.35271\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 752.8850 - val_loss: 490.8789\n",
      "Epoch 36/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 437.6823\n",
      "Epoch 36: val_loss did not improve from 234.35271\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 436.3213 - val_loss: 322.1412\n",
      "Epoch 37/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 382.4889\n",
      "Epoch 37: val_loss did not improve from 234.35271\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 388.3342 - val_loss: 314.7420\n",
      "Epoch 38/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 220.7337\n",
      "Epoch 38: val_loss did not improve from 234.35271\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 221.7315 - val_loss: 283.1917\n",
      "Epoch 39/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 451.4471\n",
      "Epoch 39: val_loss improved from 234.35271 to 173.42586, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 451.8057 - val_loss: 173.4259\n",
      "Epoch 40/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 329.3028\n",
      "Epoch 40: val_loss did not improve from 173.42586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 328.0635 - val_loss: 222.2070\n",
      "Epoch 41/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 256.5066\n",
      "Epoch 41: val_loss did not improve from 173.42586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 257.0294 - val_loss: 243.2778\n",
      "Epoch 42/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 410.4157\n",
      "Epoch 42: val_loss did not improve from 173.42586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 407.6019 - val_loss: 274.5520\n",
      "Epoch 43/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 148.0733\n",
      "Epoch 43: val_loss did not improve from 173.42586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 148.3330 - val_loss: 217.9351\n",
      "Epoch 44/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 262.2864\n",
      "Epoch 44: val_loss improved from 173.42586 to 163.17119, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 268.7610 - val_loss: 163.1712\n",
      "Epoch 45/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 179.7294\n",
      "Epoch 45: val_loss improved from 163.17119 to 129.01660, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 179.4577 - val_loss: 129.0166\n",
      "Epoch 46/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 251.0560\n",
      "Epoch 46: val_loss did not improve from 129.01660\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 256.1369 - val_loss: 240.3713\n",
      "Epoch 47/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 217.0434\n",
      "Epoch 47: val_loss improved from 129.01660 to 123.89612, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 216.2153 - val_loss: 123.8961\n",
      "Epoch 48/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 208.8143\n",
      "Epoch 48: val_loss did not improve from 123.89612\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 220.3160 - val_loss: 240.8482\n",
      "Epoch 49/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 164.8557\n",
      "Epoch 49: val_loss did not improve from 123.89612\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 165.5962 - val_loss: 171.8455\n",
      "Epoch 50/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 252.7023\n",
      "Epoch 50: val_loss improved from 123.89612 to 99.90682, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 251.4388 - val_loss: 99.9068\n",
      "Epoch 51/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 566.3893\n",
      "Epoch 51: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 564.1466 - val_loss: 309.7570\n",
      "Epoch 52/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 221.7247\n",
      "Epoch 52: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 220.7747 - val_loss: 164.4959\n",
      "Epoch 53/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 283.1925\n",
      "Epoch 53: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 285.4547 - val_loss: 176.2132\n",
      "Epoch 54/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 353.4830\n",
      "Epoch 54: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 352.3390 - val_loss: 110.3384\n",
      "Epoch 55/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 202.3955\n",
      "Epoch 55: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 203.1411 - val_loss: 148.2939\n",
      "Epoch 56/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 147.2374\n",
      "Epoch 56: val_loss did not improve from 99.90682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 153.8247 - val_loss: 399.6436\n",
      "Epoch 57/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 191.9836\n",
      "Epoch 57: val_loss improved from 99.90682 to 78.38667, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 191.0110 - val_loss: 78.3867\n",
      "Epoch 58/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 192.6530\n",
      "Epoch 58: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 191.6875 - val_loss: 84.7784\n",
      "Epoch 59/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 159.1509\n",
      "Epoch 59: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 158.7966 - val_loss: 102.2419\n",
      "Epoch 60/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 222.3416\n",
      "Epoch 60: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 223.8792 - val_loss: 507.9098\n",
      "Epoch 61/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 532.5377\n",
      "Epoch 61: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 526.6667 - val_loss: 179.1376\n",
      "Epoch 62/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 178.4611\n",
      "Epoch 62: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 180.8993 - val_loss: 99.9011\n",
      "Epoch 63/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 120.6819\n",
      "Epoch 63: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 124.8004 - val_loss: 338.0810\n",
      "Epoch 64/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1156.5569\n",
      "Epoch 64: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1140.7164 - val_loss: 124.7350\n",
      "Epoch 65/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 194.6113\n",
      "Epoch 65: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 194.5875 - val_loss: 99.0025\n",
      "Epoch 66/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 110.8036\n",
      "Epoch 66: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 111.3023 - val_loss: 86.0898\n",
      "Epoch 67/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 130.1901\n",
      "Epoch 67: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 131.3443 - val_loss: 116.3711\n",
      "Epoch 68/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 92.6547\n",
      "Epoch 68: val_loss did not improve from 78.38667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 93.2719 - val_loss: 96.9424\n",
      "Epoch 69/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 333.1425\n",
      "Epoch 69: val_loss improved from 78.38667 to 75.30299, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 329.5779 - val_loss: 75.3030\n",
      "Epoch 70/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1824.9937\n",
      "Epoch 70: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1805.8716 - val_loss: 124.6355\n",
      "Epoch 71/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 121.7333\n",
      "Epoch 71: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 121.6309 - val_loss: 153.9149\n",
      "Epoch 72/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 146.7585\n",
      "Epoch 72: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 147.2792 - val_loss: 357.4592\n",
      "Epoch 73/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 238.2022\n",
      "Epoch 73: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 236.3544 - val_loss: 95.1413\n",
      "Epoch 74/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 93.5887\n",
      "Epoch 74: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 94.2402 - val_loss: 205.5826\n",
      "Epoch 75/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 183.2920\n",
      "Epoch 75: val_loss did not improve from 75.30299\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 185.3120 - val_loss: 150.7686\n",
      "Epoch 76/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 274.2576\n",
      "Epoch 76: val_loss improved from 75.30299 to 67.21405, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 272.7792 - val_loss: 67.2141\n",
      "Epoch 77/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 94.7136\n",
      "Epoch 77: val_loss did not improve from 67.21405\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 94.8538 - val_loss: 217.5353\n",
      "Epoch 78/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 105.0176\n",
      "Epoch 78: val_loss improved from 67.21405 to 61.13461, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 104.9261 - val_loss: 61.1346\n",
      "Epoch 79/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 185.5450\n",
      "Epoch 79: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 183.7933 - val_loss: 69.8764\n",
      "Epoch 80/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 80.7876\n",
      "Epoch 80: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 83.6346 - val_loss: 79.6041\n",
      "Epoch 81/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 593.4627\n",
      "Epoch 81: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 600.0356 - val_loss: 379.6898\n",
      "Epoch 82/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 522.3745\n",
      "Epoch 82: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 525.7434 - val_loss: 248.0323\n",
      "Epoch 83/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 713.9895\n",
      "Epoch 83: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 722.7162 - val_loss: 491.8953\n",
      "Epoch 84/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 411.2167\n",
      "Epoch 84: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 411.3372 - val_loss: 590.6481\n",
      "Epoch 85/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 213.9403\n",
      "Epoch 85: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 213.5894 - val_loss: 150.0409\n",
      "Epoch 86/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 129.7195\n",
      "Epoch 86: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 131.9417 - val_loss: 94.1773\n",
      "Epoch 87/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 272.6558\n",
      "Epoch 87: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 273.4778 - val_loss: 104.5167\n",
      "Epoch 88/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 112.4896\n",
      "Epoch 88: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 112.4949 - val_loss: 94.5862\n",
      "Epoch 89/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 213.4695\n",
      "Epoch 89: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 212.5316 - val_loss: 93.4584\n",
      "Epoch 90/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 155.3152\n",
      "Epoch 90: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 155.3734 - val_loss: 160.6070\n",
      "Epoch 91/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 120.0671\n",
      "Epoch 91: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 119.5481 - val_loss: 76.6523\n",
      "Epoch 92/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 203.6873\n",
      "Epoch 92: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 205.0472 - val_loss: 99.2588\n",
      "Epoch 93/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 179.1058\n",
      "Epoch 93: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 181.8612 - val_loss: 196.6073\n",
      "Epoch 94/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 158.0837\n",
      "Epoch 94: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 157.3318 - val_loss: 112.5666\n",
      "Epoch 95/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 124.6480\n",
      "Epoch 95: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 126.0694 - val_loss: 276.6872\n",
      "Epoch 96/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 322.9076\n",
      "Epoch 96: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 325.3134 - val_loss: 104.0265\n",
      "Epoch 97/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 143.0023\n",
      "Epoch 97: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 142.2028 - val_loss: 109.0204\n",
      "Epoch 98/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 124.9754\n",
      "Epoch 98: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 127.1374 - val_loss: 434.0802\n",
      "Epoch 99/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 149.4716\n",
      "Epoch 99: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 149.3732 - val_loss: 81.7567\n",
      "Epoch 100/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 105.2279\n",
      "Epoch 100: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 104.9294 - val_loss: 79.6759\n",
      "Epoch 101/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 85.4349\n",
      "Epoch 101: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 85.9981 - val_loss: 61.7288\n",
      "Epoch 102/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1594.8422\n",
      "Epoch 102: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1579.6083 - val_loss: 115.4018\n",
      "Epoch 103/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 138.9939\n",
      "Epoch 103: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 139.0201 - val_loss: 239.1346\n",
      "Epoch 104/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 111.1866\n",
      "Epoch 104: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 111.2159 - val_loss: 72.9019\n",
      "Epoch 105/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 63.5976\n",
      "Epoch 105: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 64.2791 - val_loss: 138.6921\n",
      "Epoch 106/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 137.1916\n",
      "Epoch 106: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 136.7646 - val_loss: 70.4658\n",
      "Epoch 107/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 86.2850\n",
      "Epoch 107: val_loss did not improve from 61.13461\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 86.6580 - val_loss: 428.7045\n",
      "Epoch 108/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 210.1056\n",
      "Epoch 108: val_loss improved from 61.13461 to 59.90052, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 207.6721 - val_loss: 59.9005\n",
      "Epoch 109/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 66.1353\n",
      "Epoch 109: val_loss did not improve from 59.90052\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 67.5809 - val_loss: 184.5839\n",
      "Epoch 110/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 146.2159\n",
      "Epoch 110: val_loss did not improve from 59.90052\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 145.6463 - val_loss: 74.5611\n",
      "Epoch 111/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 197.4515\n",
      "Epoch 111: val_loss improved from 59.90052 to 58.74260, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 196.6289 - val_loss: 58.7426\n",
      "Epoch 112/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 98.1098\n",
      "Epoch 112: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 98.3983 - val_loss: 58.8785\n",
      "Epoch 113/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 82.3909\n",
      "Epoch 113: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 82.1042 - val_loss: 65.4554\n",
      "Epoch 114/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 124.9860\n",
      "Epoch 114: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 124.1735 - val_loss: 64.4924\n",
      "Epoch 115/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 113.4766\n",
      "Epoch 115: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 112.8964 - val_loss: 890.9673\n",
      "Epoch 116/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 273.1487\n",
      "Epoch 116: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 270.6066 - val_loss: 65.9664\n",
      "Epoch 117/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 527.6085\n",
      "Epoch 117: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 523.2092 - val_loss: 151.5095\n",
      "Epoch 118/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 132.7768\n",
      "Epoch 118: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 132.9173 - val_loss: 67.9721\n",
      "Epoch 119/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 91.0320\n",
      "Epoch 119: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 93.1321 - val_loss: 153.9682\n",
      "Epoch 120/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 153.6870\n",
      "Epoch 120: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 153.0518 - val_loss: 650.6211\n",
      "Epoch 121/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 294.0237\n",
      "Epoch 121: val_loss did not improve from 58.74260\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 294.8541 - val_loss: 202.6180\n",
      "Epoch 122/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 133.2654\n",
      "Epoch 122: val_loss improved from 58.74260 to 54.42383, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 133.5795 - val_loss: 54.4238\n",
      "Epoch 123/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 65.8851\n",
      "Epoch 123: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 66.9191 - val_loss: 60.7540\n",
      "Epoch 124/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 89.7233\n",
      "Epoch 124: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 90.8542 - val_loss: 100.2539\n",
      "Epoch 125/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 478.1321\n",
      "Epoch 125: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 473.2176 - val_loss: 122.8727\n",
      "Epoch 126/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 69.9967\n",
      "Epoch 126: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 71.1608 - val_loss: 78.6568\n",
      "Epoch 127/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 118.6618\n",
      "Epoch 127: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 118.4615 - val_loss: 70.0621\n",
      "Epoch 128/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 71.5680\n",
      "Epoch 128: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 72.0235 - val_loss: 72.7972\n",
      "Epoch 129/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 124.1960\n",
      "Epoch 129: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 123.2681 - val_loss: 67.6332\n",
      "Epoch 130/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 95.1912\n",
      "Epoch 130: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 94.6830 - val_loss: 71.7570\n",
      "Epoch 131/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 98.3003\n",
      "Epoch 131: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 100.3621 - val_loss: 74.2788\n",
      "Epoch 132/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 72.9899\n",
      "Epoch 132: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 72.9078 - val_loss: 64.1095\n",
      "Epoch 133/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 83.5378\n",
      "Epoch 133: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 83.3227 - val_loss: 128.6442\n",
      "Epoch 134/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1760.3743\n",
      "Epoch 134: val_loss did not improve from 54.42383\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 1730.6323 - val_loss: 92.3303\n",
      "Epoch 135/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 172.8342\n",
      "Epoch 135: val_loss improved from 54.42383 to 54.17970, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 171.8058 - val_loss: 54.1797\n",
      "Epoch 136/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 84.1835\n",
      "Epoch 136: val_loss did not improve from 54.17970\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 84.6562 - val_loss: 55.8044\n",
      "Epoch 137/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 121.1164\n",
      "Epoch 137: val_loss did not improve from 54.17970\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 120.5195 - val_loss: 79.2766\n",
      "Epoch 138/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 83.1588\n",
      "Epoch 138: val_loss improved from 54.17970 to 51.09064, saving model to best_model.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 83.1711 - val_loss: 51.0906\n",
      "Epoch 139/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 43.0086\n",
      "Epoch 139: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 44.1788 - val_loss: 87.7316\n",
      "Epoch 140/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 159.1244\n",
      "Epoch 140: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 158.2447 - val_loss: 71.5105\n",
      "Epoch 141/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 49.3839\n",
      "Epoch 141: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 49.8723 - val_loss: 646.0850\n",
      "Epoch 142/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 274.3628\n",
      "Epoch 142: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 270.4081 - val_loss: 159.2241\n",
      "Epoch 143/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 296.8589\n",
      "Epoch 143: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 302.6073 - val_loss: 135.7787\n",
      "Epoch 144/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 188.5819\n",
      "Epoch 144: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 187.7188 - val_loss: 82.0020\n",
      "Epoch 145/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 77.1160\n",
      "Epoch 145: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 76.8010 - val_loss: 69.9022\n",
      "Epoch 146/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 167.9764\n",
      "Epoch 146: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 166.3344 - val_loss: 215.8162\n",
      "Epoch 147/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 85.2560\n",
      "Epoch 147: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 85.8471 - val_loss: 134.6261\n",
      "Epoch 148/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 84.6241\n",
      "Epoch 148: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 84.3445 - val_loss: 67.9892\n",
      "Epoch 149/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 126.7163\n",
      "Epoch 149: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 128.8535 - val_loss: 223.5546\n",
      "Epoch 150/150\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 88.6915\n",
      "Epoch 150: val_loss did not improve from 51.09064\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 88.5666 - val_loss: 366.6446\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,epochs=150, batch_size=25, verbose=1, validation_data=(x_test,y_test), callbacks= model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
