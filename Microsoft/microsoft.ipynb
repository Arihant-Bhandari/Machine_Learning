{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"MSFT(2000-2023).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2000</td>\n",
       "      <td>58.687500</td>\n",
       "      <td>59.312500</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>58.281250</td>\n",
       "      <td>36.132248</td>\n",
       "      <td>53228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2000</td>\n",
       "      <td>56.781250</td>\n",
       "      <td>58.562500</td>\n",
       "      <td>56.125000</td>\n",
       "      <td>56.312500</td>\n",
       "      <td>34.911709</td>\n",
       "      <td>54119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2000</td>\n",
       "      <td>55.562500</td>\n",
       "      <td>58.187500</td>\n",
       "      <td>54.687500</td>\n",
       "      <td>56.906250</td>\n",
       "      <td>35.279816</td>\n",
       "      <td>64059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2000</td>\n",
       "      <td>56.093750</td>\n",
       "      <td>56.937500</td>\n",
       "      <td>54.187500</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>34.098019</td>\n",
       "      <td>54976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/7/2000</td>\n",
       "      <td>54.312500</td>\n",
       "      <td>56.125000</td>\n",
       "      <td>53.656250</td>\n",
       "      <td>55.718750</td>\n",
       "      <td>34.543629</td>\n",
       "      <td>62013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>11/16/2023</td>\n",
       "      <td>370.959991</td>\n",
       "      <td>376.350006</td>\n",
       "      <td>370.179993</td>\n",
       "      <td>376.170013</td>\n",
       "      <td>376.170013</td>\n",
       "      <td>27182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>11/17/2023</td>\n",
       "      <td>373.609985</td>\n",
       "      <td>374.369995</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>369.850006</td>\n",
       "      <td>369.850006</td>\n",
       "      <td>40157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>11/20/2023</td>\n",
       "      <td>371.220001</td>\n",
       "      <td>378.869995</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>377.440002</td>\n",
       "      <td>377.440002</td>\n",
       "      <td>52465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>11/21/2023</td>\n",
       "      <td>375.670013</td>\n",
       "      <td>376.220001</td>\n",
       "      <td>371.119995</td>\n",
       "      <td>373.070007</td>\n",
       "      <td>373.070007</td>\n",
       "      <td>28423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>11/22/2023</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>379.790009</td>\n",
       "      <td>374.970001</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>23345300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6012 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0       1/3/2000   58.687500   59.312500   56.000000   58.281250   36.132248   \n",
       "1       1/4/2000   56.781250   58.562500   56.125000   56.312500   34.911709   \n",
       "2       1/5/2000   55.562500   58.187500   54.687500   56.906250   35.279816   \n",
       "3       1/6/2000   56.093750   56.937500   54.187500   55.000000   34.098019   \n",
       "4       1/7/2000   54.312500   56.125000   53.656250   55.718750   34.543629   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6007  11/16/2023  370.959991  376.350006  370.179993  376.170013  376.170013   \n",
       "6008  11/17/2023  373.609985  374.369995  367.000000  369.850006  369.850006   \n",
       "6009  11/20/2023  371.220001  378.869995  371.000000  377.440002  377.440002   \n",
       "6010  11/21/2023  375.670013  376.220001  371.119995  373.070007  373.070007   \n",
       "6011  11/22/2023  378.000000  379.790009  374.970001  377.850006  377.850006   \n",
       "\n",
       "        Volume  \n",
       "0     53228400  \n",
       "1     54119000  \n",
       "2     64059600  \n",
       "3     54976600  \n",
       "4     62013600  \n",
       "...        ...  \n",
       "6007  27182300  \n",
       "6008  40157000  \n",
       "6009  52465100  \n",
       "6010  28423100  \n",
       "6011  23345300  \n",
       "\n",
       "[6012 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2000</td>\n",
       "      <td>58.68750</td>\n",
       "      <td>59.3125</td>\n",
       "      <td>56.00000</td>\n",
       "      <td>58.28125</td>\n",
       "      <td>36.132248</td>\n",
       "      <td>53228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2000</td>\n",
       "      <td>56.78125</td>\n",
       "      <td>58.5625</td>\n",
       "      <td>56.12500</td>\n",
       "      <td>56.31250</td>\n",
       "      <td>34.911709</td>\n",
       "      <td>54119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2000</td>\n",
       "      <td>55.56250</td>\n",
       "      <td>58.1875</td>\n",
       "      <td>54.68750</td>\n",
       "      <td>56.90625</td>\n",
       "      <td>35.279816</td>\n",
       "      <td>64059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2000</td>\n",
       "      <td>56.09375</td>\n",
       "      <td>56.9375</td>\n",
       "      <td>54.18750</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>34.098019</td>\n",
       "      <td>54976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/7/2000</td>\n",
       "      <td>54.31250</td>\n",
       "      <td>56.1250</td>\n",
       "      <td>53.65625</td>\n",
       "      <td>55.71875</td>\n",
       "      <td>34.543629</td>\n",
       "      <td>62013600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      Open     High       Low     Close  Adj Close    Volume\n",
       "0  1/3/2000  58.68750  59.3125  56.00000  58.28125  36.132248  53228400\n",
       "1  1/4/2000  56.78125  58.5625  56.12500  56.31250  34.911709  54119000\n",
       "2  1/5/2000  55.56250  58.1875  54.68750  56.90625  35.279816  64059600\n",
       "3  1/6/2000  56.09375  56.9375  54.18750  55.00000  34.098019  54976600\n",
       "4  1/7/2000  54.31250  56.1250  53.65625  55.71875  34.543629  62013600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>11/16/2023</td>\n",
       "      <td>370.959991</td>\n",
       "      <td>376.350006</td>\n",
       "      <td>370.179993</td>\n",
       "      <td>376.170013</td>\n",
       "      <td>376.170013</td>\n",
       "      <td>27182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>11/17/2023</td>\n",
       "      <td>373.609985</td>\n",
       "      <td>374.369995</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>369.850006</td>\n",
       "      <td>369.850006</td>\n",
       "      <td>40157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>11/20/2023</td>\n",
       "      <td>371.220001</td>\n",
       "      <td>378.869995</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>377.440002</td>\n",
       "      <td>377.440002</td>\n",
       "      <td>52465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>11/21/2023</td>\n",
       "      <td>375.670013</td>\n",
       "      <td>376.220001</td>\n",
       "      <td>371.119995</td>\n",
       "      <td>373.070007</td>\n",
       "      <td>373.070007</td>\n",
       "      <td>28423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>11/22/2023</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>379.790009</td>\n",
       "      <td>374.970001</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>23345300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "6007  11/16/2023  370.959991  376.350006  370.179993  376.170013  376.170013   \n",
       "6008  11/17/2023  373.609985  374.369995  367.000000  369.850006  369.850006   \n",
       "6009  11/20/2023  371.220001  378.869995  371.000000  377.440002  377.440002   \n",
       "6010  11/21/2023  375.670013  376.220001  371.119995  373.070007  373.070007   \n",
       "6011  11/22/2023  378.000000  379.790009  374.970001  377.850006  377.850006   \n",
       "\n",
       "        Volume  \n",
       "6007  27182300  \n",
       "6008  40157000  \n",
       "6009  52465100  \n",
       "6010  28423100  \n",
       "6011  23345300  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6012 entries, 0 to 6011\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       6012 non-null   object \n",
      " 1   Open       6012 non-null   float64\n",
      " 2   High       6012 non-null   float64\n",
      " 3   Low        6012 non-null   float64\n",
      " 4   Close      6012 non-null   float64\n",
      " 5   Adj Close  6012 non-null   float64\n",
      " 6   Volume     6012 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 328.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6012.000000</td>\n",
       "      <td>6012.000000</td>\n",
       "      <td>6012.000000</td>\n",
       "      <td>6012.000000</td>\n",
       "      <td>6012.000000</td>\n",
       "      <td>6.012000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>77.767658</td>\n",
       "      <td>78.582541</td>\n",
       "      <td>76.941528</td>\n",
       "      <td>77.786900</td>\n",
       "      <td>70.789050</td>\n",
       "      <td>5.132949e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.008302</td>\n",
       "      <td>87.897979</td>\n",
       "      <td>86.087633</td>\n",
       "      <td>87.036507</td>\n",
       "      <td>88.438452</td>\n",
       "      <td>3.074841e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.200000</td>\n",
       "      <td>15.620000</td>\n",
       "      <td>14.870000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>11.304623</td>\n",
       "      <td>7.425600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.030001</td>\n",
       "      <td>27.299999</td>\n",
       "      <td>26.807499</td>\n",
       "      <td>27.049999</td>\n",
       "      <td>19.027981</td>\n",
       "      <td>2.865732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.550001</td>\n",
       "      <td>32.997500</td>\n",
       "      <td>32.187500</td>\n",
       "      <td>32.635000</td>\n",
       "      <td>23.633584</td>\n",
       "      <td>4.605860e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>84.222498</td>\n",
       "      <td>84.727497</td>\n",
       "      <td>83.497500</td>\n",
       "      <td>84.192499</td>\n",
       "      <td>78.474423</td>\n",
       "      <td>6.504930e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>378.000000</td>\n",
       "      <td>379.790009</td>\n",
       "      <td>374.970001</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>377.850006</td>\n",
       "      <td>5.910522e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close    Adj Close  \\\n",
       "count  6012.000000  6012.000000  6012.000000  6012.000000  6012.000000   \n",
       "mean     77.767658    78.582541    76.941528    77.786900    70.789050   \n",
       "std      87.008302    87.897979    86.087633    87.036507    88.438452   \n",
       "min      15.200000    15.620000    14.870000    15.150000    11.304623   \n",
       "25%      27.030001    27.299999    26.807499    27.049999    19.027981   \n",
       "50%      32.550001    32.997500    32.187500    32.635000    23.633584   \n",
       "75%      84.222498    84.727497    83.497500    84.192499    78.474423   \n",
       "max     378.000000   379.790009   374.970001   377.850006   377.850006   \n",
       "\n",
       "             Volume  \n",
       "count  6.012000e+03  \n",
       "mean   5.132949e+07  \n",
       "std    3.074841e+07  \n",
       "min    7.425600e+06  \n",
       "25%    2.865732e+07  \n",
       "50%    4.605860e+07  \n",
       "75%    6.504930e+07  \n",
       "max    5.910522e+08  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('closing price of stock')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Closing price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['Date'])):\n",
    "    date_parts = df['Date'][i].split(\"/\")\n",
    "    day, month, year = date_parts[0], date_parts[1], date_parts[2]\n",
    "    \n",
    "    # Add leading zeros if necessary\n",
    "    day = day.zfill(2)\n",
    "    month = month.zfill(2)\n",
    "    \n",
    "    formatted_date = f\"{year}{month}{day}\"\n",
    "    df['Date'][i] = int(formatted_date)\n",
    "\n",
    "df['Date'] = df['Date'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6012 entries, 0 to 6011\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       6012 non-null   int64  \n",
      " 1   Open       6012 non-null   float64\n",
      " 2   High       6012 non-null   float64\n",
      " 3   Low        6012 non-null   float64\n",
      " 4   Close      6012 non-null   float64\n",
      " 5   Adj Close  6012 non-null   float64\n",
      " 6   Volume     6012 non-null   int64  \n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 328.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy().drop(columns=['Open'],axis=1)\n",
    "y = df.copy()[['Open']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "x = scale.fit_transform(x,y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape before reshaping : (4809, 6)\n",
      "y_train shape before reshaping : (4809, 1)\n",
      "x_test shape before reshaping : (1203, 6)\n",
      "y_test shape before reshaping : (1203, 1)\n",
      "x_train shape after reshaping : (4809, 6, 1)\n",
      "x_test shape after reshaping : (1203, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape before reshaping :\", x_train.shape)\n",
    "print(\"y_train shape before reshaping :\", y_train.shape)\n",
    "print(\"x_test shape before reshaping :\", x_test.shape)\n",
    "print(\"y_test shape before reshaping :\", y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "print(\"x_train shape after reshaping :\", x_train.shape)\n",
    "print(\"x_test shape after reshaping :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,800</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,440</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">168,800</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,150</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,100</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m50\u001b[0m)          â”‚        \u001b[38;5;34m10,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m75\u001b[0m)          â”‚        \u001b[38;5;34m37,800\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_20 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)         â”‚        \u001b[38;5;34m70,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m10\u001b[0m)          â”‚         \u001b[38;5;34m4,440\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_22 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            â”‚       \u001b[38;5;34m168,800\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_16 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            â”‚        \u001b[38;5;34m30,150\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_17 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚        \u001b[38;5;34m15,100\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             â”‚         \u001b[38;5;34m5,050\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m51\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">342,191</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m342,191\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">342,191</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m342,191\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,ELU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=True, activation='relu', input_shape = (x_train.shape[1],1)))\n",
    "model.add(LSTM(75,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(100,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(10,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(200,return_sequences=False, activation='relu'))\n",
    "model.add(Dense(150, activation=ELU()))\n",
    "model.add(Dense(100, activation=ELU()))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7186.3315\n",
      "Epoch 1: val_loss improved from inf to 102.92527, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - loss: 7143.4077 - val_loss: 102.9253\n",
      "Epoch 2/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 89.1760\n",
      "Epoch 2: val_loss did not improve from 102.92527\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 89.1418 - val_loss: 106.4505\n",
      "Epoch 3/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 73.2517\n",
      "Epoch 3: val_loss improved from 102.92527 to 53.37316, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 73.1822 - val_loss: 53.3732\n",
      "Epoch 4/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 50.0806\n",
      "Epoch 4: val_loss improved from 53.37316 to 43.92580, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 50.1241 - val_loss: 43.9258\n",
      "Epoch 5/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 56.5373\n",
      "Epoch 5: val_loss improved from 43.92580 to 41.19337, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 56.4662 - val_loss: 41.1934\n",
      "Epoch 6/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 43.6325\n",
      "Epoch 6: val_loss did not improve from 41.19337\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 43.6166 - val_loss: 49.7002\n",
      "Epoch 7/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 47.9818\n",
      "Epoch 7: val_loss did not improve from 41.19337\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 48.1027 - val_loss: 54.8945\n",
      "Epoch 8/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 37.9197\n",
      "Epoch 8: val_loss improved from 41.19337 to 37.38790, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 37.8735 - val_loss: 37.3879\n",
      "Epoch 9/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 31.1890\n",
      "Epoch 9: val_loss improved from 37.38790 to 27.28126, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 31.1821 - val_loss: 27.2813\n",
      "Epoch 10/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 36.6066\n",
      "Epoch 10: val_loss did not improve from 27.28126\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 36.6348 - val_loss: 46.8550\n",
      "Epoch 11/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 31.5893\n",
      "Epoch 11: val_loss did not improve from 27.28126\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 31.6015 - val_loss: 37.1238\n",
      "Epoch 12/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 31.2499\n",
      "Epoch 12: val_loss did not improve from 27.28126\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 31.2666 - val_loss: 34.1124\n",
      "Epoch 13/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 30.2136\n",
      "Epoch 13: val_loss improved from 27.28126 to 21.83604, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 30.1878 - val_loss: 21.8360\n",
      "Epoch 14/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 28.6936\n",
      "Epoch 14: val_loss did not improve from 21.83604\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 28.7454 - val_loss: 22.2719\n",
      "Epoch 15/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 26.8625\n",
      "Epoch 15: val_loss did not improve from 21.83604\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 26.9241 - val_loss: 51.9951\n",
      "Epoch 16/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 25.9357\n",
      "Epoch 16: val_loss did not improve from 21.83604\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 25.9096 - val_loss: 36.3207\n",
      "Epoch 17/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 27.4536\n",
      "Epoch 17: val_loss did not improve from 21.83604\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 27.4411 - val_loss: 240.4415\n",
      "Epoch 18/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 37.2664\n",
      "Epoch 18: val_loss did not improve from 21.83604\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 37.0020 - val_loss: 29.6374\n",
      "Epoch 19/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 21.6931\n",
      "Epoch 19: val_loss improved from 21.83604 to 16.99456, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 21.7169 - val_loss: 16.9946\n",
      "Epoch 20/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 24.0205\n",
      "Epoch 20: val_loss did not improve from 16.99456\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 24.0418 - val_loss: 27.7358\n",
      "Epoch 21/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 23.7211\n",
      "Epoch 21: val_loss improved from 16.99456 to 16.25956, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 23.7194 - val_loss: 16.2596\n",
      "Epoch 22/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 24.2570\n",
      "Epoch 22: val_loss did not improve from 16.25956\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 24.2146 - val_loss: 127.8202\n",
      "Epoch 23/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 28.1553\n",
      "Epoch 23: val_loss did not improve from 16.25956\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 28.1284 - val_loss: 63.6269\n",
      "Epoch 24/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 21.3937\n",
      "Epoch 24: val_loss improved from 16.25956 to 15.72059, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 21.3920 - val_loss: 15.7206\n",
      "Epoch 25/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 27.1098\n",
      "Epoch 25: val_loss improved from 15.72059 to 14.24795, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 27.0764 - val_loss: 14.2479\n",
      "Epoch 26/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 18.2207\n",
      "Epoch 26: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 18.2918 - val_loss: 26.3470\n",
      "Epoch 27/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 24.3599\n",
      "Epoch 27: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 24.3501 - val_loss: 20.2017\n",
      "Epoch 28/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 16.8328\n",
      "Epoch 28: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 16.8129 - val_loss: 60.2703\n",
      "Epoch 29/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 16.1270\n",
      "Epoch 29: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 16.1399 - val_loss: 26.1978\n",
      "Epoch 30/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 16.5788\n",
      "Epoch 30: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 16.5601 - val_loss: 52.5174\n",
      "Epoch 31/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 33.7779\n",
      "Epoch 31: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 33.6492 - val_loss: 399.8056\n",
      "Epoch 32/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 34.4102\n",
      "Epoch 32: val_loss did not improve from 14.24795\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 34.2482 - val_loss: 40.7998\n",
      "Epoch 33/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 16.6605\n",
      "Epoch 33: val_loss improved from 14.24795 to 12.04437, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 16.6327 - val_loss: 12.0444\n",
      "Epoch 34/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13.8183\n",
      "Epoch 34: val_loss did not improve from 12.04437\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 13.8104 - val_loss: 13.9434\n",
      "Epoch 35/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13.3338\n",
      "Epoch 35: val_loss improved from 12.04437 to 9.94879, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 13.4269 - val_loss: 9.9488\n",
      "Epoch 36/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 10.6147\n",
      "Epoch 36: val_loss improved from 9.94879 to 7.47941, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 10.6114 - val_loss: 7.4794\n",
      "Epoch 37/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11.1801\n",
      "Epoch 37: val_loss did not improve from 7.47941\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 11.2026 - val_loss: 11.5250\n",
      "Epoch 38/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 10.2919\n",
      "Epoch 38: val_loss did not improve from 7.47941\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 10.3041 - val_loss: 8.1934\n",
      "Epoch 39/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 24.0361\n",
      "Epoch 39: val_loss did not improve from 7.47941\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 24.1119 - val_loss: 14.2659\n",
      "Epoch 40/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12.9557\n",
      "Epoch 40: val_loss did not improve from 7.47941\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 12.9182 - val_loss: 9.0458\n",
      "Epoch 41/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.2543\n",
      "Epoch 41: val_loss improved from 7.47941 to 6.45608, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 9.2635 - val_loss: 6.4561\n",
      "Epoch 42/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.3752\n",
      "Epoch 42: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.3516 - val_loss: 8.8143\n",
      "Epoch 43/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.2015\n",
      "Epoch 43: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.1945 - val_loss: 9.5824\n",
      "Epoch 44/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 10.5799\n",
      "Epoch 44: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 10.5787 - val_loss: 21.0871\n",
      "Epoch 45/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.2967\n",
      "Epoch 45: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 7.3127 - val_loss: 1018.3448\n",
      "Epoch 46/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 76.1529\n",
      "Epoch 46: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 75.6436 - val_loss: 12.5443\n",
      "Epoch 47/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.3664\n",
      "Epoch 47: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 9.3779 - val_loss: 25.1983\n",
      "Epoch 48/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12.4917\n",
      "Epoch 48: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 12.4116 - val_loss: 6.7184\n",
      "Epoch 49/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.0213\n",
      "Epoch 49: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 9.0199 - val_loss: 139.7234\n",
      "Epoch 50/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15.1276\n",
      "Epoch 50: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 15.0742 - val_loss: 140.3979\n",
      "Epoch 51/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14.6642\n",
      "Epoch 51: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 14.5886 - val_loss: 9.3383\n",
      "Epoch 52/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.4511\n",
      "Epoch 52: val_loss did not improve from 6.45608\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 6.4644 - val_loss: 7.8658\n",
      "Epoch 53/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9954\n",
      "Epoch 53: val_loss improved from 6.45608 to 3.62672, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 5.9855 - val_loss: 3.6267\n",
      "Epoch 54/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 10.4463\n",
      "Epoch 54: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 10.4196 - val_loss: 12.9673\n",
      "Epoch 55/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9208\n",
      "Epoch 55: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 5.9426 - val_loss: 25.6545\n",
      "Epoch 56/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.0253\n",
      "Epoch 56: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 6.0247 - val_loss: 3.8831\n",
      "Epoch 57/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.4345\n",
      "Epoch 57: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.4611 - val_loss: 843.4482\n",
      "Epoch 58/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 107.8658\n",
      "Epoch 58: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 107.0730 - val_loss: 12.9773\n",
      "Epoch 59/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.6655\n",
      "Epoch 59: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.6653 - val_loss: 3.8376\n",
      "Epoch 60/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.7316\n",
      "Epoch 60: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.7335 - val_loss: 4.3820\n",
      "Epoch 61/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.3734\n",
      "Epoch 61: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.3315 - val_loss: 3.6624\n",
      "Epoch 62/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.7163\n",
      "Epoch 62: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.7224 - val_loss: 32.5518\n",
      "Epoch 63/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.4712\n",
      "Epoch 63: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 5.4497 - val_loss: 15.7786\n",
      "Epoch 64/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.4600\n",
      "Epoch 64: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.4692 - val_loss: 4.4067\n",
      "Epoch 65/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.5382\n",
      "Epoch 65: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.5341 - val_loss: 5.0967\n",
      "Epoch 66/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.0296\n",
      "Epoch 66: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 8.0219 - val_loss: 11.6882\n",
      "Epoch 67/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.1340\n",
      "Epoch 67: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 9.0905 - val_loss: 75.9240\n",
      "Epoch 68/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 51.0624\n",
      "Epoch 68: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 50.7250 - val_loss: 4.0706\n",
      "Epoch 69/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.5558\n",
      "Epoch 69: val_loss did not improve from 3.62672\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.5590 - val_loss: 19.6868\n",
      "Epoch 70/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9756\n",
      "Epoch 70: val_loss improved from 3.62672 to 3.26989, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 5.9650 - val_loss: 3.2699\n",
      "Epoch 71/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.4254\n",
      "Epoch 71: val_loss did not improve from 3.26989\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 5.4181 - val_loss: 3.6779\n",
      "Epoch 72/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.7412\n",
      "Epoch 72: val_loss did not improve from 3.26989\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.7392 - val_loss: 24.1542\n",
      "Epoch 73/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.5409\n",
      "Epoch 73: val_loss did not improve from 3.26989\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 8.5162 - val_loss: 6.7901\n",
      "Epoch 74/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.6634\n",
      "Epoch 74: val_loss did not improve from 3.26989\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 3.6699 - val_loss: 5.1272\n",
      "Epoch 75/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.3893\n",
      "Epoch 75: val_loss improved from 3.26989 to 2.78619, saving model to best_model.keras\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.3930 - val_loss: 2.7862\n",
      "Epoch 76/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.4452\n",
      "Epoch 76: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.4723 - val_loss: 6.0621\n",
      "Epoch 77/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 11.1563\n",
      "Epoch 77: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 11.1263 - val_loss: 7.5740\n",
      "Epoch 78/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.2877\n",
      "Epoch 78: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 7.2584 - val_loss: 13.5594\n",
      "Epoch 79/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.6813\n",
      "Epoch 79: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.6691 - val_loss: 5.0456\n",
      "Epoch 80/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.3675\n",
      "Epoch 80: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.4237 - val_loss: 14.0483\n",
      "Epoch 81/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.2085\n",
      "Epoch 81: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.2035 - val_loss: 3.4284\n",
      "Epoch 82/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.5747\n",
      "Epoch 82: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 9.5148 - val_loss: 3.1440\n",
      "Epoch 83/100\n",
      "\u001b[1m95/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.6654\n",
      "Epoch 83: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.6712 - val_loss: 24.5754\n",
      "Epoch 84/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.7272\n",
      "Epoch 84: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.6845 - val_loss: 5.1125\n",
      "Epoch 85/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.2036\n",
      "Epoch 85: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 6.1935 - val_loss: 39.7353\n",
      "Epoch 86/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11.2947\n",
      "Epoch 86: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 11.2527 - val_loss: 9.8774\n",
      "Epoch 87/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.6992\n",
      "Epoch 87: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.7001 - val_loss: 21.1821\n",
      "Epoch 88/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.1889\n",
      "Epoch 88: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.1361 - val_loss: 4.4874\n",
      "Epoch 89/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.7856\n",
      "Epoch 89: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.7867 - val_loss: 10.6159\n",
      "Epoch 90/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.3330\n",
      "Epoch 90: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.3431 - val_loss: 15.7969\n",
      "Epoch 91/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.0706\n",
      "Epoch 91: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 6.0579 - val_loss: 41.4914\n",
      "Epoch 92/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.2689\n",
      "Epoch 92: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 5.2628 - val_loss: 156.6622\n",
      "Epoch 93/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 26.4581\n",
      "Epoch 93: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 26.1215 - val_loss: 3.0681\n",
      "Epoch 94/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3098\n",
      "Epoch 94: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.3363 - val_loss: 350.9743\n",
      "Epoch 95/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 29.5904\n",
      "Epoch 95: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 29.1893 - val_loss: 5.6025\n",
      "Epoch 96/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.9109\n",
      "Epoch 96: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.9111 - val_loss: 33.3098\n",
      "Epoch 97/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.5025\n",
      "Epoch 97: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 5.4910 - val_loss: 5.5362\n",
      "Epoch 98/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.3748\n",
      "Epoch 98: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.3747 - val_loss: 8.7855\n",
      "Epoch 99/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.0195\n",
      "Epoch 99: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 6.9961 - val_loss: 22.6443\n",
      "Epoch 100/100\n",
      "\u001b[1m96/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.1396\n",
      "Epoch 100: val_loss did not improve from 2.78619\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 6.1163 - val_loss: 6.3498\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,epochs=100, batch_size=50, verbose=1, validation_data=(x_test,y_test), callbacks= model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy().drop(columns=['Close'],axis=1)\n",
    "y = df.copy()[['Close']]\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "x = scale.fit_transform(x,y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape before reshaping : (4509, 6)\n",
      "y_train shape before reshaping : (4509, 1)\n",
      "x_test shape before reshaping : (1503, 6)\n",
      "y_test shape before reshaping : (1503, 1)\n",
      "x_train shape after reshaping : (4509, 6, 1)\n",
      "x_test shape after reshaping : (1503, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape before reshaping :\", x_train.shape)\n",
    "print(\"y_train shape before reshaping :\", y_train.shape)\n",
    "print(\"x_test shape before reshaping :\", x_test.shape)\n",
    "print(\"y_test shape before reshaping :\", y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "print(\"x_train shape after reshaping :\", x_train.shape)\n",
    "print(\"x_test shape after reshaping :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,800</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,440</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">168,800</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,150</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,100</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m50\u001b[0m)          â”‚        \u001b[38;5;34m10,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_24 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m75\u001b[0m)          â”‚        \u001b[38;5;34m37,800\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_25 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)         â”‚        \u001b[38;5;34m70,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_26 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m10\u001b[0m)          â”‚         \u001b[38;5;34m4,440\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_27 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            â”‚       \u001b[38;5;34m168,800\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            â”‚        \u001b[38;5;34m30,150\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_21 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚        \u001b[38;5;34m15,100\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_22 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             â”‚         \u001b[38;5;34m5,050\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m51\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">342,191</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m342,191\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">342,191</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m342,191\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,ELU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=True, activation='relu', input_shape = (x_train.shape[1],1)))\n",
    "model.add(LSTM(75,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(100,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(10,return_sequences=True, activation=ELU()))\n",
    "model.add(LSTM(200,return_sequences=False, activation='relu'))\n",
    "model.add(Dense(150, activation=ELU()))\n",
    "model.add(Dense(100, activation=ELU()))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6743.7041\n",
      "Epoch 1: val_loss improved from inf to 103.80132, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 37ms/step - loss: 6702.1895 - val_loss: 103.8013\n",
      "Epoch 2/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 87.0581\n",
      "Epoch 2: val_loss improved from 103.80132 to 72.28568, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 87.0267 - val_loss: 72.2857\n",
      "Epoch 3/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 65.6920\n",
      "Epoch 3: val_loss did not improve from 72.28568\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 65.6321 - val_loss: 80.3233\n",
      "Epoch 4/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 52.1067\n",
      "Epoch 4: val_loss improved from 72.28568 to 44.87727, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 52.0483 - val_loss: 44.8773\n",
      "Epoch 5/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 43.2793\n",
      "Epoch 5: val_loss did not improve from 44.87727\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 43.2593 - val_loss: 410.2221\n",
      "Epoch 6/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 81.1415\n",
      "Epoch 6: val_loss improved from 44.87727 to 35.33977, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 80.7949 - val_loss: 35.3398\n",
      "Epoch 7/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 38.1391\n",
      "Epoch 7: val_loss improved from 35.33977 to 28.53261, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 38.1550 - val_loss: 28.5326\n",
      "Epoch 8/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 38.8232\n",
      "Epoch 8: val_loss did not improve from 28.53261\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 38.7350 - val_loss: 49.1970\n",
      "Epoch 9/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 35.2244\n",
      "Epoch 9: val_loss did not improve from 28.53261\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 35.2226 - val_loss: 35.2574\n",
      "Epoch 10/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 42.4408\n",
      "Epoch 10: val_loss improved from 28.53261 to 23.61929, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 42.4768 - val_loss: 23.6193\n",
      "Epoch 11/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 29.7773\n",
      "Epoch 11: val_loss did not improve from 23.61929\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 29.7701 - val_loss: 32.2921\n",
      "Epoch 12/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 30.4344\n",
      "Epoch 12: val_loss did not improve from 23.61929\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 30.4384 - val_loss: 33.6814\n",
      "Epoch 13/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 31.3844\n",
      "Epoch 13: val_loss did not improve from 23.61929\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 31.3448 - val_loss: 61.1986\n",
      "Epoch 14/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 42.4788\n",
      "Epoch 14: val_loss improved from 23.61929 to 20.41678, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 42.3856 - val_loss: 20.4168\n",
      "Epoch 15/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 38.9303\n",
      "Epoch 15: val_loss did not improve from 20.41678\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 38.6526 - val_loss: 35.6782\n",
      "Epoch 16/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 29.9789\n",
      "Epoch 16: val_loss did not improve from 20.41678\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 29.9742 - val_loss: 26.0177\n",
      "Epoch 17/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 26.9766\n",
      "Epoch 17: val_loss did not improve from 20.41678\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 26.9780 - val_loss: 22.4822\n",
      "Epoch 18/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 21.5883\n",
      "Epoch 18: val_loss improved from 20.41678 to 19.45333, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 21.7150 - val_loss: 19.4533\n",
      "Epoch 19/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 20.0710\n",
      "Epoch 19: val_loss did not improve from 19.45333\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 20.0507 - val_loss: 75.9784\n",
      "Epoch 20/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 20.6981\n",
      "Epoch 20: val_loss improved from 19.45333 to 16.47341, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 20.6794 - val_loss: 16.4734\n",
      "Epoch 21/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15.7232\n",
      "Epoch 21: val_loss did not improve from 16.47341\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 15.7629 - val_loss: 53.4968\n",
      "Epoch 22/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 24.5544\n",
      "Epoch 22: val_loss improved from 16.47341 to 8.06757, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 24.5760 - val_loss: 8.0676\n",
      "Epoch 23/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 10.2869\n",
      "Epoch 23: val_loss improved from 8.06757 to 6.39724, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 10.3023 - val_loss: 6.3972\n",
      "Epoch 24/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15.2379\n",
      "Epoch 24: val_loss improved from 6.39724 to 5.28190, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 15.3136 - val_loss: 5.2819\n",
      "Epoch 25/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.9426\n",
      "Epoch 25: val_loss improved from 5.28190 to 3.66314, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 9.9757 - val_loss: 3.6631\n",
      "Epoch 26/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.1332\n",
      "Epoch 26: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 5.1580 - val_loss: 22.8655\n",
      "Epoch 27/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.7926\n",
      "Epoch 27: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 8.8763 - val_loss: 4.0151\n",
      "Epoch 28/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.7588\n",
      "Epoch 28: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 4.7725 - val_loss: 5.3836\n",
      "Epoch 29/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.8836\n",
      "Epoch 29: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.8876 - val_loss: 19.6089\n",
      "Epoch 30/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 10.4141\n",
      "Epoch 30: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 10.4614 - val_loss: 5.7256\n",
      "Epoch 31/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14.8414\n",
      "Epoch 31: val_loss did not improve from 3.66314\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 14.8329 - val_loss: 4.5622\n",
      "Epoch 32/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.3902\n",
      "Epoch 32: val_loss improved from 3.66314 to 2.07310, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 9.3415 - val_loss: 2.0731\n",
      "Epoch 33/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.1931\n",
      "Epoch 33: val_loss improved from 2.07310 to 1.89328, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 5.2510 - val_loss: 1.8933\n",
      "Epoch 34/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.1788\n",
      "Epoch 34: val_loss did not improve from 1.89328\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.2143 - val_loss: 2.0875\n",
      "Epoch 35/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1283\n",
      "Epoch 35: val_loss did not improve from 1.89328\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 7.1213 - val_loss: 5.2291\n",
      "Epoch 36/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.4612\n",
      "Epoch 36: val_loss improved from 1.89328 to 1.22487, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 4.4314 - val_loss: 1.2249\n",
      "Epoch 37/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.4865\n",
      "Epoch 37: val_loss did not improve from 1.22487\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.5147 - val_loss: 2.5234\n",
      "Epoch 38/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.8823\n",
      "Epoch 38: val_loss did not improve from 1.22487\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 3.9257 - val_loss: 640.3666\n",
      "Epoch 39/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 96.4473\n",
      "Epoch 39: val_loss did not improve from 1.22487\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 94.3772 - val_loss: 6.5349\n",
      "Epoch 40/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15.4130\n",
      "Epoch 40: val_loss did not improve from 1.22487\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 15.3328 - val_loss: 5.8042\n",
      "Epoch 41/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.9207\n",
      "Epoch 41: val_loss did not improve from 1.22487\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 3.9119 - val_loss: 2.0091\n",
      "Epoch 42/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3724\n",
      "Epoch 42: val_loss improved from 1.22487 to 1.16995, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 3.3758 - val_loss: 1.1699\n",
      "Epoch 43/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7639\n",
      "Epoch 43: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.9137 - val_loss: 31.9711\n",
      "Epoch 44/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.2490\n",
      "Epoch 44: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 7.2516 - val_loss: 3.5518\n",
      "Epoch 45/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.7982\n",
      "Epoch 45: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2.7924 - val_loss: 1.7134\n",
      "Epoch 46/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.4333\n",
      "Epoch 46: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 9.4371 - val_loss: 3.3128\n",
      "Epoch 47/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5916\n",
      "Epoch 47: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.5997 - val_loss: 8.0067\n",
      "Epoch 48/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.9201\n",
      "Epoch 48: val_loss did not improve from 1.16995\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.9106 - val_loss: 2.5992\n",
      "Epoch 49/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2865\n",
      "Epoch 49: val_loss improved from 1.16995 to 0.90998, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 2.2905 - val_loss: 0.9100\n",
      "Epoch 50/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1437\n",
      "Epoch 50: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.1963 - val_loss: 21.2126\n",
      "Epoch 51/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.7110\n",
      "Epoch 51: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 5.7033 - val_loss: 1.4384\n",
      "Epoch 52/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12.4625\n",
      "Epoch 52: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 12.5869 - val_loss: 4.1838\n",
      "Epoch 53/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.4657\n",
      "Epoch 53: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 7.4862 - val_loss: 14.7027\n",
      "Epoch 54/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2349\n",
      "Epoch 54: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2.2334 - val_loss: 15.3291\n",
      "Epoch 55/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1159\n",
      "Epoch 55: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 6.0788 - val_loss: 1.4416\n",
      "Epoch 56/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.2443\n",
      "Epoch 56: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 5.2899 - val_loss: 2.1944\n",
      "Epoch 57/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1154\n",
      "Epoch 57: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.1096 - val_loss: 2.9302\n",
      "Epoch 58/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.2610\n",
      "Epoch 58: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 8.2372 - val_loss: 4.1148\n",
      "Epoch 59/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12.3335\n",
      "Epoch 59: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 12.2206 - val_loss: 2.9652\n",
      "Epoch 60/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9552\n",
      "Epoch 60: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.9702 - val_loss: 1.0747\n",
      "Epoch 61/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2654\n",
      "Epoch 61: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2.2681 - val_loss: 1.0338\n",
      "Epoch 62/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8182\n",
      "Epoch 62: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.8511 - val_loss: 370.1285\n",
      "Epoch 63/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 87.8819\n",
      "Epoch 63: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 86.5159 - val_loss: 1.2889\n",
      "Epoch 64/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6060\n",
      "Epoch 64: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.6025 - val_loss: 2.0218\n",
      "Epoch 65/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8695\n",
      "Epoch 65: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.8703 - val_loss: 1.1011\n",
      "Epoch 66/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.7608\n",
      "Epoch 66: val_loss did not improve from 0.90998\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.7193 - val_loss: 52.3259\n",
      "Epoch 67/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 19.2962\n",
      "Epoch 67: val_loss improved from 0.90998 to 0.79457, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 19.1604 - val_loss: 0.7946\n",
      "Epoch 68/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8277\n",
      "Epoch 68: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.8316 - val_loss: 22.7946\n",
      "Epoch 69/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.0656\n",
      "Epoch 69: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2.0620 - val_loss: 42.3033\n",
      "Epoch 70/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6563\n",
      "Epoch 70: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.6274 - val_loss: 54.3260\n",
      "Epoch 71/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13.9835\n",
      "Epoch 71: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 14.0064 - val_loss: 6.4531\n",
      "Epoch 72/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.5255\n",
      "Epoch 72: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 3.4915 - val_loss: 1.2084\n",
      "Epoch 73/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4172\n",
      "Epoch 73: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1.4146 - val_loss: 1.1962\n",
      "Epoch 74/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3518\n",
      "Epoch 74: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.3684 - val_loss: 1.2210\n",
      "Epoch 75/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7114\n",
      "Epoch 75: val_loss did not improve from 0.79457\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.7130 - val_loss: 1.0736\n",
      "Epoch 76/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.9857\n",
      "Epoch 76: val_loss improved from 0.79457 to 0.69182, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.9862 - val_loss: 0.6918\n",
      "Epoch 77/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.8379\n",
      "Epoch 77: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.8586 - val_loss: 3.7493\n",
      "Epoch 78/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1028\n",
      "Epoch 78: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 6.0726 - val_loss: 5.7087\n",
      "Epoch 79/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.0060\n",
      "Epoch 79: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.9964 - val_loss: 2.9093\n",
      "Epoch 80/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2938\n",
      "Epoch 80: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.2977 - val_loss: 17.9015\n",
      "Epoch 81/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14.8075\n",
      "Epoch 81: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 14.8225 - val_loss: 1.3047\n",
      "Epoch 82/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2502\n",
      "Epoch 82: val_loss did not improve from 0.69182\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.2517 - val_loss: 2.8664\n",
      "Epoch 83/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0787\n",
      "Epoch 83: val_loss improved from 0.69182 to 0.62238, saving model to best_model.keras\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1.0762 - val_loss: 0.6224\n",
      "Epoch 84/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.9449\n",
      "Epoch 84: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.0021 - val_loss: 1.2989\n",
      "Epoch 85/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.8958\n",
      "Epoch 85: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.9007 - val_loss: 2.1247\n",
      "Epoch 86/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3118\n",
      "Epoch 86: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.3336 - val_loss: 0.9528\n",
      "Epoch 87/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.9471\n",
      "Epoch 87: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.9493 - val_loss: 5.8158\n",
      "Epoch 88/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.8979\n",
      "Epoch 88: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.9341 - val_loss: 1.9203\n",
      "Epoch 89/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5157\n",
      "Epoch 89: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.5226 - val_loss: 3.4374\n",
      "Epoch 90/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6852\n",
      "Epoch 90: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1.6971 - val_loss: 27.4019\n",
      "Epoch 91/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6012\n",
      "Epoch 91: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.6302 - val_loss: 47.7363\n",
      "Epoch 92/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 27.4322\n",
      "Epoch 92: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 26.8139 - val_loss: 1.2944\n",
      "Epoch 93/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.7982\n",
      "Epoch 93: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.8007 - val_loss: 0.8454\n",
      "Epoch 94/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0194\n",
      "Epoch 94: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.0189 - val_loss: 0.6466\n",
      "Epoch 95/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3592\n",
      "Epoch 95: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.3369 - val_loss: 3.5048\n",
      "Epoch 96/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.3644\n",
      "Epoch 96: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.3545 - val_loss: 1.7136\n",
      "Epoch 97/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2927\n",
      "Epoch 97: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.3059 - val_loss: 13.3268\n",
      "Epoch 98/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1755\n",
      "Epoch 98: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 6.0764 - val_loss: 1.9546\n",
      "Epoch 99/100\n",
      "\u001b[1m90/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7836\n",
      "Epoch 99: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2.7875 - val_loss: 1.0040\n",
      "Epoch 100/100\n",
      "\u001b[1m89/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7652\n",
      "Epoch 100: val_loss did not improve from 0.62238\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1.7869 - val_loss: 0.7399\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,epochs=100, batch_size=50, verbose=1, validation_data=(x_test,y_test), callbacks= model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
